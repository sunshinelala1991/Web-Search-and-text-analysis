{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fb64f22f4b9d6a57ea7261cf771e646e223fad9c85d255dda4a7efa4"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "signature": "dbe5c8ee5ef597fc2680bec06e686a52a01edd44992269bd65c07d09"
   },
   "source": [
    "Student Name: Yun Wang\n",
    "Student ID: 672323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5653cf33f236b021076096666f0eaca598a8b6ea818c0d2a6425a15b"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eac8f7335062ea69220caf0a93873dbcb9121a0c0d8bafc349dbbfff"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "55678c4756c3c6d4aa908b80503b61a10c423f53cd814b52ad443b54"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "9c1902745267a3e3d99862e5ec6431ab9f901f75baa61ef5a024a968"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "signature": "d206ed2a6dd099cad03569db61a8b4b5de7e6ec1c4590e340a588a93"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "word_set=set(nltk.corpus.words.words())\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "# count the number of tweets required to print\n",
    "num_tweet_print=1\n",
    "# a list used to hold the original tweets(in json format)\n",
    "original_tweet=[]\n",
    "'''\n",
    "Preprocess a single tweet.\n",
    "The method takes a single tweet as input, remove all the '@'s and urls at first, \n",
    "then find all the hashtags in this tweet and store them into a list. Then the hashtags \n",
    "are tokenized based on whether there is capticalization. If yes, it is splitted using the capital \n",
    "letters, if not, it is splitted using maxMatch algorithm-- lemmatize the substring first, then \n",
    "look it up in the word_set(nltk.corpus.words.words()). After this, all the hashtags are removed \n",
    "from tweet and the tweet is tokenized into sentences then words. Finally, the list of words inside\n",
    "the hashtag is added to the end of the list of words from tweet and returned.\n",
    "'''\n",
    "def preprocess(ori_tweet):\n",
    "    global num_tweet_print\n",
    "    # get rid of all the \"@\"s and urls\n",
    "    tweet = re.sub(\"@[^ ]+\", \"\", ori_tweet).strip()\n",
    "    tweet=re.sub(\"http[^ ]+\",\"\",tweet).strip()\n",
    "    # Find all the hashtags and put them in a list called hashtag\n",
    "    hashtag = re.findall('#[^ ]*', tweet)\n",
    "    tag_list=[]\n",
    "    if hashtag:\n",
    "        for tag in hashtag:\n",
    "            #remove the \"#\" from one hashtag\n",
    "            tag=re.sub('#','',tag)\n",
    "            # if contains capital letters\n",
    "            if re.search('.*[A-Z].*',tag):\n",
    "                tag_list=tag_list+re.findall('[A-Z][^A-Z]*',tag)\n",
    "\n",
    "            else:\n",
    "                 # if not contain capital letters use a maxmatch algorithm to find the words\n",
    "                i=0\n",
    "                while i<len(tag):\n",
    "                    for j in range(len(tag),i,-1):\n",
    "                        #lemmatize words first\n",
    "                        lemma=lemmatize(tag[i:j])\n",
    "                        if lemma in word_set:\n",
    "                            tag_list.append(tag[i:j])\n",
    "                            i=j-1\n",
    "                    i+=1\n",
    "    if tag_list:\n",
    "        for i in range(len(tag_list)):\n",
    "            tag_list[i]=tag_list[i].lower()\n",
    "    #remove all the hashtags\n",
    "    tweet = re.sub('#[^ ]*','',tweet).lower()\n",
    "    #split tweet into sentences\n",
    "    tweet = sent_segmenter.tokenize(tweet)\n",
    "    #put the words in a tweet into a words list\n",
    "    words = []\n",
    "    for sentence in tweet:\n",
    "        words= words+ word_tokenizer.tokenize(sentence)\n",
    "    #if the tweet has hashtags, add them to the words list too\n",
    "    if tag_list:\n",
    "        words=words+tag_list\n",
    "    if len(tag_list)>1:\n",
    "        if num_tweet_print<11:\n",
    "            print num_tweet_print\n",
    "            print ori_tweet\n",
    "            print words\n",
    "            num_tweet_print+=1\n",
    "\n",
    "    return words\n",
    "\n",
    "'''\n",
    "Given the file name, pre-process the file and return a list of preprocessed tweet \n",
    "and label lists\n",
    "'''\n",
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels=[]\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        original_tweet.append(tweet_dict)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels\n",
    "'''\n",
    "lemmatize a word. First treat the word as a verb, if not then treat the word as a noun\n",
    "'''\n",
    "def lemmatize(word):\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7dc1fa62f9e403c31f91caba69d41d98516063cde6b0d1ced8de92df"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "81d5303fd413fafb373f77293320f3b0aca065df23c6f4a55043585e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft\n",
      "[u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'.', u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'?', u'windows', u'x', u'box', u'one']\n",
      "2\n",
      "@MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail\n",
      "[u'i', u'will', u'be', u'downgrading', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'trying', u'it', u'again', u'.', u'windows10', u'windows10fail']\n",
      "3\n",
      "For the 1st time @Skype has a \"High Startup impact\" Does anyone at @Microsoft have a clue?#Windows10Fail http://t.co/loO3yd5rwe\n",
      "[u'for', u'the', u'1st', u'time', u'has', u'a', u'\"', u'high', u'startup', u'impact', u'\"', u'does', u'anyone', u'at', u'have', u'a', u'clue', u'?', u'windows10', u'fail']\n",
      "4\n",
      "#teens @BillGates 1st company failed miserably. When Gates & @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "[u'1st', u'company', u'failed', u'miserably', u'.', u'when', u'gates', u'&', u'tried', u'to', u'sell', u'the', u'product', u'it', u'wouldn', u\"'\", u't', u'work', u'teens', u'never', u'give', u'up']\n",
      "5\n",
      "#Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "[u'for', u'to', u'become', u'the', u'10th', u'global', u'non', u'profit', u'partner', u'of', u'for', u'us', u'to', u'together', u'.', u'vote', u'upgrade', u'your', u'world']\n",
      "6\n",
      "Top 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n",
      "[u'top', u'5', u'most', u'searched', u'for', u'back', u'-', u'to', u'-', u'school', u'topics', u'--', u'the', u'list', u'may', u'surprise', u'you', u'back', u'to', u'school']\n",
      "7\n",
      "@taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n",
      "[u'we', u'have', u'an', u'iot', u'workshop', u'by', u'at', u'11pm', u'on', u'the', u'friday', u'-', u'definitely', u'worth', u'going', u'for', u'inspiration', u'!', u'hack', u'the', u'planet']\n",
      "8\n",
      "@ForbesRussia #MBA #casestudy Namaste 2 #google and @Microsoft's CEOs, but #Multiculturals mttr!May B the era of Bad Translations wd B Over?\n",
      "[u'namaste', u'2', u'and', u'ceos', u',', u'but', u'mttr', u'!', u'may', u'b', u'the', u'era', u'of', u'bad', u'translations', u'wd', u'b', u'over', u'?', u'm', u'b', u'a', u'cases', u'tu', u'd', u'y', u'goo', u'g', u'l', u'e', u'multiculturals']\n",
      "9\n",
      "After 75 minutes of being on hold with @Microsoft in India 1-800-936-5700 \"Adrian\" wants to transfer my call again(3rd time) #Windows10Fail\n",
      "[u'after', u'75', u'minutes', u'of', u'being', u'on', u'hold', u'with', u'in', u'india', u'1', u'-', u'800', u'-', u'936', u'-', u'5700', u'\"', u'adrian', u'\"', u'wants', u'to', u'transfer', u'my', u'call', u'again', u'(', u'3rd', u'time', u')', u'windows10', u'fail']\n",
      "10\n",
      "We're excited to learn about #cloud #analytics from @Microsoft tomorrow! Join us https://t.co/p0bMREBBHC #tech #rva http://t.co/1XHmPdSvzq\n",
      "[u'we', u\"'\", u're', u'excited', u'to', u'learn', u'about', u'from', u'tomorrow', u'!', u'join', u'us', u'cloud', u'analytics', u'tech', u'r', u'v', u'a']\n"
     ]
    }
   ],
   "source": [
    "trn_tweets,trn_labels=preprocess_file('train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>My analysis</b>:\n",
    "From the above output, we can see the following errors:\n",
    "1. When the hashtag has several words in it, but does not have enough capital letters to separate them, like  #Windows10fail, our code only split this hashtag based on \"W\", thus returning Windows10fail as a single word. \n",
    "2. When the hashtag is an abbreviation, consisting of only capital letters, like #MBA, which should be recognized as a single word, but our code would split it into \"M\",\"B\",\"A\"\n",
    "3. My implementation of maxMatch matches the hashtag from left to right, for hashtags like #casestudy, the algorithm would first match 'cases' with 'case', and 'tudy' is left, no word can be matched with 'tudy'\n",
    "4. Punctuations are useless most of the times but are not removed.\n",
    "\n",
    "\n",
    "To solve problem 1 and 2, we have to include a more delicate method to tell if the hashtag is an abbreviation, or if there is not enough capital letters to split the string. In addition, many words are not included in the list of English words from NLTK, thus we may need a more comprehensive lexicon. For error 3, a more intelligent maxMatch algorithm is needed, which will match the longest word in the string first, and is able to tell whether 's' or 'ed' come from the tense of the word. For error 4, we just need to include an extra step to remove punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ec5ec021ac23aa5b89b40b7df4d6c8031e8e07cae5e2eebf80d873d0"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "signature": "dfc552b1cad57233536271c6d32c3f09dba50ff56286f6a194e25f90"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Method convert_to_feature_dicts\n",
    "Take the preprocessed list of tweets as input, remove_stop_words indicates whether to remove\n",
    "stop words, and n denotes whether to remove words appearing less than n times across the \n",
    "entire training set. \n",
    "Return a feature dictionary. \n",
    "\n",
    "First construct a dictionary using all the words in the training set, which is used to \n",
    "identify the words appearing less than n times. These words are stored into small_set. \n",
    "If remove_stop_words==True and n >0, then only words not in small_set and stop_word_set \n",
    "will be added to the feature dic.\n",
    "'''\n",
    "\n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n):\n",
    "    feature_dicts = []\n",
    "    if remove_stop_words:\n",
    "        stop_word_set=set(stopwords.words('english'))\n",
    "    if n>0:\n",
    "        small_set=set()\n",
    "        whole_feature_dic={}\n",
    "        for tweet in tweets:\n",
    "            for w in tweet:\n",
    "                whole_feature_dic[w]=whole_feature_dic.get(w,0)+1\n",
    "\n",
    "        for w in whole_feature_dic:\n",
    "            if whole_feature_dic[w]<=n:\n",
    "                small_set.add(w)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        # build feature dictionary for tweet\n",
    "        feature_dict={}\n",
    "        for w in tweet:\n",
    "\n",
    "            if remove_stop_words and n<=0:\n",
    "                if w not in stop_word_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif remove_stop_words and n>0:\n",
    "                if w not in stop_word_set and w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            elif n>0:\n",
    "                if w not in small_set:\n",
    "                    feature_dict[w]=feature_dict.get(w,0)+1\n",
    "            else:\n",
    "                feature_dict[w]=feature_dict.get(w,0)+1\n",
    "\n",
    "        feature_dicts.append(feature_dict)\n",
    "\n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "932fa4ffe864b9155e1d31b112ec463cd0c11684f77071cfeaea72f7"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "00083e185628eaf2ebd60c081534bb750f4755b7a683f5c2dcea11ce"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "signature": "20ef700ae3f07171f04d1e3d5cdf837b36e17f9dcf1f80881b767a0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#proprocess training data\n",
    "trn_tweets,trn_labels=preprocess_file('train.json')\n",
    "# convert training data into feature dictionaries\n",
    "trn_feature_dicts=convert_to_feature_dicts(trn_tweets,True,1)\n",
    "#proprocess development data\n",
    "dev_tweets,dev_labels=preprocess_file('dev.json')\n",
    "# convert development data into feature dictionaries\n",
    "dev_feature_dicts=convert_to_feature_dicts(dev_tweets,True,0)\n",
    "\n",
    "#convert the data to the sparse representation\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def prepare_data(trn_feature_dicts,dev_feature_dicts):\n",
    "    vectorizer = DictVectorizer()\n",
    "    trn_feature_dicts = vectorizer.fit_transform(trn_feature_dicts)\n",
    "    dev_feature_dicts = vectorizer.transform(dev_feature_dicts)\n",
    "    return trn_feature_dicts,dev_feature_dicts\n",
    "\n",
    "#convert training data and development data into sparse representation\n",
    "trn_feature_dicts,dev_feature_dicts=prepare_data(trn_feature_dicts,dev_feature_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e72f22d1037544244612b2150d94b99bfbf7eb86a789835e278d779f"
   },
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook).  Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "signature": "97ad269db9c0e5026aeb2cadf02f4a0f27a0a2b85df3bf4c16720891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitter best num_split 2 accuracy 0.441224712958\n",
      "splitter best num_split 5002 accuracy 0.48277747403\n",
      "splitter best num_split 10002 accuracy 0.481683980317\n",
      "splitter best num_split 15002 accuracy 0.445598687808\n",
      "splitter best num_split 20002 accuracy 0.382722799344\n",
      "splitter random num_split 2 accuracy 0.45106615637\n",
      "splitter random num_split 5002 accuracy 0.482230727173\n",
      "splitter random num_split 10002 accuracy 0.419901585566\n",
      "splitter random num_split 15002 accuracy 0.41935483871\n",
      "splitter random num_split 20002 accuracy 0.382722799344\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCSzlKRERCSzlKRERCSz/h9LAgujDTq/kAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19ff2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "'''\n",
    "test returns the accuracy of the classifier trained on the training data\n",
    "Input:\n",
    "clf: the classifier to be used\n",
    "training_data: the training data\n",
    "training_classifications: the labels of training data\n",
    "test_data: the data used to test the accuracy of the classifier trained on training data\n",
    "test_classifications: the labels of test data\n",
    "Return: accuracy\n",
    "'''\n",
    "def test(clf,training_data,training_classifications,test_data,test_classifications):\n",
    "\n",
    "    clf.fit(training_data,training_classifications)\n",
    "    predictions = clf.predict(test_data)\n",
    "    accuracy = accuracy_score(test_classifications,predictions)\n",
    "    return accuracy\n",
    "\n",
    "'''\n",
    "plot_graph plots the accuracy results of decision tree classifier\n",
    "Input: \n",
    "accuracy_list is used to store the accuracies\n",
    "range_list indicates the list of numbers of min_samples_split, which is parameter for decision\n",
    "tree classifier\n",
    "'''\n",
    "def plot_graph(accuracy_list,category_list,range_list):\n",
    "    n_groups = 5\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.4\n",
    "    rects1 = plt.bar(index, accuracy_list[0:5], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     label=category_list[0])\n",
    "    rects2 = plt.bar(index + bar_width, accuracy_list[5:10], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     label=category_list[1])\n",
    "\n",
    "    plt.xlabel('Classifier parameters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy using different classifier parameters')\n",
    "    plt.xticks(index + bar_width, range_list)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Train the classifier with different parameter settings. In this assignment,\n",
    "the parameter splitter(The strategy used to choose the split at each node. \n",
    "Supported strategies are “best” to choose the best split and “random” to \n",
    "choose the best random split.) and min_samples_split(The minimum number of \n",
    "samples required to split an internal node.) are examined. Splitter is set to\n",
    "'best' and 'random', while min_samples_split is set to range(2,25002,5000)\n",
    "'''\n",
    "\n",
    "accuracy_list=[]\n",
    "splitters=['best','random']\n",
    "num_splits=range(2,25002,5000)\n",
    "for split_method in splitters:\n",
    "    for num_split in num_splits:\n",
    "        clf1=DecisionTreeClassifier(splitter=split_method,min_samples_split=num_split)\n",
    "\n",
    "        acc= test(clf1,trn_feature_dicts,trn_labels,dev_feature_dicts,dev_labels)\n",
    "        print 'splitter',split_method,'num_split',num_split,'accuracy', acc\n",
    "        accuracy_list.append(acc)\n",
    "        \n",
    "plot_graph(accuracy_list,splitters,num_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "The parameters tuned are splitter and min_samples_split.\n",
    "\n",
    "From the above graph, we can see that 'best' mostly performs better than 'random', and num_split also have much influence on the final result.  \n",
    "\n",
    "For the splitter parameter, best is the default for the decision tree. For each feature, splitter parameter tries to get the best split. There is another option called random for the splitter parameter, random chooses the best random split, this means that it chooses the threshold randomly instead of optimizing the theta(it chooses the random threshold theta). Random parameter is more useful when it comes to ensemble learning, as it allows trees to look different. In our case, since there is only one decision tree, random turns out to be not very useful and mostly gives worse performance than best.\n",
    "\n",
    "min_samples_split is the minimum number of samples required to split an internal node. It is used to control the number of samples at a leaf node. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. In our case, when the min_samples_split equals to 2 or when min_samples_split is too large such as 15002 or 20002, the classifier gives poor performance. min_smaples_split=2 leads to overfit while min_samples_split=15002 or min_samples_split=20002 leads to underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3586fbaaae10742a1f14cf65912fd539eb723b13e25d419ea1f91f49"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "signature": "73eed72d2b12d09b05c29be321905bf8eaad8b8c009e3586dca40162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver newton-cg max_iter 1 accuracy 0.488791689448\n",
      "solver newton-cg max_iter 21 accuracy 0.501366867141\n",
      "solver newton-cg max_iter 41 accuracy 0.501366867141\n",
      "solver newton-cg max_iter 61 accuracy 0.501366867141\n",
      "solver newton-cg max_iter 81 accuracy 0.501366867141\n",
      "solver lbfgs max_iter 1 accuracy 0.474576271186\n",
      "solver lbfgs max_iter 21 accuracy 0.487698195735\n",
      "solver lbfgs max_iter 41 accuracy 0.501913613997\n",
      "solver lbfgs max_iter 61 accuracy 0.503553854565\n",
      "solver lbfgs max_iter 81 accuracy 0.502460360853\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3d9d17b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "'''\n",
    "Build logistic regression model using different parameter settings, i.e. \n",
    "max_iter(Maximum number of iterations taken for the solvers to converge)\n",
    "and penalty(str, ‘l1’ or ‘l2’,Used to specify the norm used in the penalization).\n",
    "For this assignment, penalty is set to 'l1' and 'l2', while max_iter is set to 10,100,\n",
    "1000,10000,100000\n",
    "'''\n",
    "solvers=['newton-cg', 'lbfgs']\n",
    "max_iter_range=range(1,101,20)\n",
    "accuracy_list2=[]\n",
    "for asolver in solvers:\n",
    "    for max_iter in max_iter_range:\n",
    "        clf2=LogisticRegression(solver=asolver,max_iter=max_iter)\n",
    "        clf2.fit(trn_feature_dicts,trn_labels)\n",
    "        predictions_clf2=clf2.predict(dev_feature_dicts)\n",
    "        acc= accuracy_score(dev_labels,predictions_clf2)\n",
    "        #print classification_report(dev_labels,predictions_clf2)\n",
    "        print 'solver',asolver,'max_iter',max_iter,'accuracy', acc\n",
    "        accuracy_list2.append(acc)\n",
    "plot_graph(accuracy_list2,solvers,max_iter_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b> \n",
    "The parameter tuned for logisitc regression is solver and max_iter.\n",
    "\n",
    "We examined two solvers, namely, 'newton-cg' and 'lbfgs' since only these two solvers handle multinomial loss while the other two solvers, 'liblinear' and 'sag' are limited to one-versus-rest schemes. As our tweets have 3 class labels--positive, negative and neutral, so this problem should be considered as a multinomial classification and we assume that 'newton-cg' and 'lbfgs' gives better results than the other two. From the above graph, we can see that when both the classifiers(using newton-cg and lbfgs) converge, they give similar accuracy results. However, it appears that 'newton-cg' takes less iteration to converge. (Sometimes 'newton-cg'may also fail to converge, but still give 50% accuracy)\n",
    "\n",
    "As for the max_iter parameter, which controls the upper limit of iterations, when the number is too small, the classifiers could not converge, thus giving poor performance. Nonetheless, in some cases, we may come across the situation where the dataset is too large and only a very small amount of time is allowed for training, then setting the max_iter smaller may be practical and help us get results faster on the cost of losing some performance.\n",
    "\n",
    "<b>Comparison of two classifiers</b>\n",
    "The best performance of these two classifiers are similar, with decision tree at 48% and logistic regression slightly higher, at 50%. It is evident that logistic regression classifier is better at tweet sentiment analysis task that decision tree. The most common class baseline is 42%, and our classifiers are merely 8 percent better than this figure, which indicates that the classifiers are not performing very well and this is no doubt a challenging task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "88bc514c88161b16a2531906d60a0851ce0d940169b86ba3d7414918"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "signature": "81a3035ebbbd8625ede32260be722cf4ca25d8df6d80fab8578017fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my logistic regression\n",
      "[ 0  0 -1 -1  1]\n",
      "logistic regression\n",
      "[ 0  0 -1 -1  1]\n",
      "my logistic regression\n",
      "[[ 0.07687803  0.91108103  0.01204093]\n",
      " [ 0.0016897   0.93759511  0.06071519]\n",
      " [ 0.55849734  0.13954131  0.30196135]\n",
      " [ 0.66383418  0.26359286  0.07257297]\n",
      " [ 0.04644524  0.372929    0.58062576]]\n",
      "logistic regression\n",
      "[[ 0.07687803  0.91108103  0.01204093]\n",
      " [ 0.0016897   0.93759511  0.06071519]\n",
      " [ 0.55849734  0.13954131  0.30196135]\n",
      " [ 0.66383418  0.26359286  0.07257297]\n",
      " [ 0.04644524  0.372929    0.58062576]]\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "The logistic regression class implemented by myself. \n",
    "It uses the calculated coefficents and intercepts to predict the probability \n",
    "for different classes, or give the label of the class with highest probability \n",
    "score.\n",
    "'''\n",
    "class MyLogisticRegression:\n",
    "    '''\n",
    "    initialize the class with calculated coefficients, intercepts, and labels of the training \n",
    "    data\n",
    "    '''\n",
    "    def __init__(self, weights, constants, labels):\n",
    "        self.weights=weights\n",
    "        self.constants=constants\n",
    "        self.labels=labels\n",
    "    \n",
    "    '''\n",
    "    predict_proba takes the training data(which is already converted to a sparse matrix) as input,\n",
    "    do the calculations and return an array of probabilities, where each row corresponds to the\n",
    "    probabilities for all the classes for a specific tweet.\n",
    "    '''\n",
    "    def predict_proba(self,X):\n",
    "        prob=[]\n",
    "        X=X.toarray()\n",
    "        for index,x in enumerate(X):\n",
    "            numerator=[]\n",
    "            i=0\n",
    "            for weight in self.weights:\n",
    "                numerator.append(np.exp(np.dot(x, weight)+self.constants[i]))\n",
    "                i+=1\n",
    "            denominator=sum(numerator)\n",
    "            prob.append([])\n",
    "\n",
    "            for nume in numerator:\n",
    "                prob[index].append(nume/denominator)\n",
    "        return np.asarray(prob)\n",
    "    \n",
    "    '''\n",
    "    predict method takes the training data(which is already converted to a sparse matrix) as \n",
    "    input and returns the class label with the highest probability.\n",
    "    '''\n",
    "\n",
    "    def predict(self,X):\n",
    "        label_list=[]\n",
    "        X=X.toarray()\n",
    "        for index,x in enumerate(X):\n",
    "            numerator=[]\n",
    "            i=0\n",
    "            for weight in self.weights:\n",
    "                numerator.append(np.exp(np.dot(x, weight)+self.constants[i]))\n",
    "                i+=1\n",
    "            denominator=sum(numerator)\n",
    "            best_prob=-9e99\n",
    "            best_label=None\n",
    "            for index2,nume in enumerate(numerator):\n",
    "                class_prob=nume/denominator\n",
    "                if class_prob > best_prob:\n",
    "                    best_prob=class_prob\n",
    "                    best_label=self.labels[index2]\n",
    "            label_list.append(best_label)\n",
    "        return np.asarray(label_list)\n",
    "\n",
    "# Train a classifier using the training data\n",
    "clf3 = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf3.fit(trn_feature_dicts,trn_labels)\n",
    "#Get the classifier built by our own\n",
    "myLR=MyLogisticRegression(clf3.coef_, clf3.intercept_, clf3.classes_)\n",
    "\n",
    "#To compare my results with the scikit learn classifier\n",
    "print 'my logistic regression'\n",
    "print myLR.predict(dev_feature_dicts)[0:5]\n",
    "print 'logistic regression'\n",
    "print clf3.predict(dev_feature_dicts)[0:5]\n",
    "\n",
    "print 'my logistic regression'\n",
    "print myLR.predict_proba(dev_feature_dicts)[0:5]\n",
    "print 'logistic regression'\n",
    "print clf3.predict_proba(dev_feature_dicts)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Analysis</b>\n",
    "From the above result, we can see that the two classifiers give the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7eb4249c43c16b198796115b2b1977a30e1f32cf0e8efde44391938f"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "222499934251174c01259d47f9aa0b77fc082c539f3ec8fc74293fb6"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "signature": "89220443510ad7c018f80daf991ee34cba839e7b5eaaff0adf64ebf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. positive list negative list\n",
      "[u'gossamer', u'Mighty_Mouse', u'magnetic', u'munificence', u'idealised']\n",
      "[u'funereal', u'unscientific', u'foul', u'aggression', u'ill_nature']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "positive_list1=[]\n",
    "negative_list1=[]\n",
    "\n",
    "#count = 0\n",
    "for synset in wn.all_synsets():\n",
    "    #count += 1\n",
    "    #if count % 1000 == 0:\n",
    "        #print count\n",
    "    # count synset polarity for each lemma\n",
    "    name=synset.name()\n",
    "\n",
    "    polarity_type=get_polarity_type(name)\n",
    "    if polarity_type is not None:\n",
    "        if polarity_type ==1:\n",
    "            # if the polarity type of the synset name is 1\n",
    "            # add the lemma to the positive list\n",
    "            positive_list1+=synset.lemma_names()\n",
    "        elif polarity_type== -1:\n",
    "            # if the polarity type of the synset name is -1\n",
    "            # add the lemma to the negative list\n",
    "            negative_list1+=synset.lemma_names()\n",
    "\n",
    "positive_list1=list(set(positive_list1))\n",
    "negative_list1=list(set(negative_list1))\n",
    "print '1. positive list negative list'\n",
    "print positive_list1[0:5]\n",
    "print negative_list1[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the above positive list and negative list, we can see that all the words in the positive list can be related to something positive. If they are adjectives, they can be used to discribe something good, or if they are nouns, they are something with good quality or represent a good characteristic of a person. The negative word list is also of good quality, all of the 5 words can be related to something bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9ce3937489fc060ba10b096aae24b2b953c9fd104f244c92c97c82e0"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of ±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "signature": "9931c8c4464e5da7da0b9b5a2fea8e9cd12a5a5fcc8935a40c27ee4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2. positive list negative list\n",
      "[u'Loen', u'Pergamon', u'feasibility', u'Pampa', u'modest']\n",
      "[u'Debts', u'clotted', u'hastily', u'comically', u'disobeying']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.data import find\n",
    "\n",
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "'''\n",
    "second_lexicon takes the positive_seeds and negative_seeds as input, use the sample model from \n",
    "'models/word2vec_sample/pruned.word2vec.txt'. For every word in the model.vocab, calculate the \n",
    "model similarity of it and the word from positive or negative seed. Flip the sign for negative \n",
    "seed and sum them up, then take the average. If the score is more than 0.03, then the word is\n",
    "added to the positive list, if the score is less than 0.03, then the word is added to the negative\n",
    "list.\n",
    "'''\n",
    "def second_lexicon(positive_seeds,negative_seeds):\n",
    "\n",
    "    word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "    positive_list=[]\n",
    "    negative_list=[]\n",
    "\n",
    "    for aword in model.vocab:\n",
    "        score=0\n",
    "        for pseed in positive_seeds:\n",
    "            score+=model.similarity(aword, pseed)\n",
    "        for nseed in negative_seeds:\n",
    "            score-=model.similarity(aword,nseed)\n",
    "\n",
    "        score=score/16.0\n",
    "        if score>0.03:\n",
    "            positive_list.append(aword)\n",
    "        elif score<-0.03:\n",
    "            negative_list.append(aword)\n",
    "\n",
    "    return positive_list,negative_list\n",
    "\n",
    "positive_list2, negative_list2=second_lexicon(positive_seeds, negative_seeds)\n",
    "print ' 2. positive list negative list'\n",
    "print positive_list2[0:5]\n",
    "print negative_list2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "Positive list: [u'Loen', u'Pergamon', u'feasibility', u'Pampa', u'modest']\n",
    "\n",
    "Loen and Pergamon are names, so we actually can  not tell its polarity. Feasibility is a noun used to describe the feature of something, and can not be classified exactly as positive or negative. Pampa may be related to something good in geography or travel magazines. Modest describes a good characteris of a person.\n",
    "\n",
    "[u'Debts', u'clotted', u'hastily', u'comically', u'disobeying']\n",
    "\n",
    "The lexicon for negative words seems to be more accurate. All of the words above can be related to something bad or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0bae4e130bdd5a1c767517625a7f0b686e7fb27448d03a3f41358314"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  ±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "signature": "6dd1b963e2f08f3169f207703e2558f8f3b28e42f002125372f43d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. positive list negative list\n",
      "[u'francesca', u'comically', u'spidery', u'ultra-violet', u'non-violent']\n",
      "[u'bilharziasis', u'pigment', u'wooden', u'deferments', u'blot-appearance']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import math\n",
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "'''\n",
    "get_BOW takes a list of strings as input, return a dictionary with words as keys and the number\n",
    "of times that word occurs as values.\n",
    "'''\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "'''\n",
    "third_lexicon takes the positive seeds and negative seeds as input, and returns lists of \n",
    "positive words and negative words.\n",
    "First a dictionary all_dic is created with all the words in brown.words() as entries. Each entry is another\n",
    "dictionary with positive seeds, negative seeds and 'word_count' as keys. The values of positive\n",
    "seed keys represent the the co-occurence times of that specific word with the positive seed, and\n",
    "likewise for negative seeds. The value in 'word_count' represents the number of times that word \n",
    "occur throughout the document. Another dicitionary constructed is seed_total_dic, it stores the \n",
    "number of times each seed occurs in the document. \n",
    "For the co-occurence calculation, multiple co-occurences in the same text is calculated as one.\n",
    "\n",
    "The 0 and negative PMI scores are discarded, and the signs of PMI scores with negative seeds are \n",
    "flipped. The sum is calculated and then divided by 16 to get the average. This figure is then \n",
    "compared to +0.3,-0.3 to add words to positive list and negative list. \n",
    "'''\n",
    "def third_lexicon(positive_seeds,negative_seeds):\n",
    "    positive_list=[]\n",
    "    negative_list=[]\n",
    "\n",
    "    all_dic={}\n",
    "    seed_total_dic={}\n",
    "    for fileid in brown.fileids():\n",
    "        bow=get_BOW(brown.words(fileid))\n",
    "        for aword in bow:\n",
    "            all_dic[aword]=all_dic.get(aword,{})\n",
    "            all_dic[aword]['word_count']=all_dic[aword].get('word_count',0)+1\n",
    "            for pseed in positive_seeds:\n",
    "                if pseed in bow:\n",
    "                    all_dic[aword][pseed]=all_dic[aword].get(pseed,0)+1\n",
    "            for nseed in negative_seeds:\n",
    "                if nseed in bow:\n",
    "                    all_dic[aword][nseed]=all_dic[aword].get(nseed,0)+1\n",
    "\n",
    "        for pseed in positive_seeds:\n",
    "            if pseed in bow:\n",
    "                seed_total_dic[pseed]=seed_total_dic.get(pseed,0)+1\n",
    "        for nseed in negative_seeds:\n",
    "            if nseed in bow:\n",
    "                seed_total_dic[nseed]=seed_total_dic.get(nseed,0)+1\n",
    "\n",
    "    total_count=float(len(brown.fileids()))\n",
    "\n",
    "    for aword in all_dic:\n",
    "        score=0\n",
    "        for pseed in positive_seeds:\n",
    "            if all_dic[aword].get(pseed) != None:\n",
    "                a_score=math.log((all_dic[aword][pseed]/total_count)/((all_dic[aword]['word_count']/total_count)*(seed_total_dic[pseed]/total_count)), 2)\n",
    "                if a_score>0:\n",
    "                    score+=a_score\n",
    "\n",
    "        for nseed in negative_seeds:\n",
    "            if all_dic[aword].get(nseed) != None:\n",
    "                a_score=math.log((all_dic[aword][nseed]/total_count)/((all_dic[aword]['word_count']/total_count)*(seed_total_dic[nseed]/total_count)), 2)\n",
    "                if a_score>0:\n",
    "                    score-=a_score\n",
    "\n",
    "\n",
    "        score=score/16.0\n",
    "\n",
    "\n",
    "        if score>0.3:\n",
    "            positive_list.append(aword)\n",
    "        elif score<-0.3:\n",
    "            negative_list.append(aword)\n",
    "\n",
    "    return positive_list,negative_list\n",
    "\n",
    "positive_list3, negative_list3=third_lexicon(positive_seeds, negative_seeds)\n",
    "print '3. positive list negative list'\n",
    "print positive_list3[0:5]\n",
    "print negative_list3[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "The resulted list does not seem to be of good quality. Francesca is a name and ultra-violet is an object, they both should be neutral rather than positive. Comically and spidery are more likely to be neutral words as well. As for the negative list, pigment and wooden do not seem negative. \n",
    "This is mainly due to the fact that the PPMI scores are calculated on a text level instead of a sentence level. The words in the same text are less correlated to each other than the words in the same sentence, thus resulting in a list of bad quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "df2905143fcc1c2614933587329996421e16ea24a28c95f81d8c1a39"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "signature": "a21792bdaddd795fa32b1dc88bfdd995cd5849f8a0e06a09071b09e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon 1 percentage in opinion_lexicon.positive(): 0.272183449651\n",
      "lexicon 1 percentage in opinion_lexicon.negative(): 0.288939995819\n",
      "lexicon 2 percentage in opinion_lexicon.positive(): 0.294616151545\n",
      "lexicon 2 percentage in opinion_lexicon.negative(): 0.407275768346\n",
      "lexicon 3 percentage in opinion_lexicon.positive(): 0.0438683948156\n",
      "lexicon 3 percentage in opinion_lexicon.negative(): 0.0449508676563\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "'''\n",
    "calculate_percentage takes two lists as input, return the percentage of the overlapping words\n",
    "in the first list.\n",
    "'''\n",
    "def calculate_percentage(manual,automatic):\n",
    "    automatic=set(automatic)\n",
    "    count=0\n",
    "    for word in manual:\n",
    "        if word in automatic:\n",
    "            count+=1\n",
    "    return float(count)/len(manual)\n",
    "\n",
    "\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "print 'lexicon 1 percentage in opinion_lexicon.positive():',calculate_percentage(positive_words,positive_list1)\n",
    "print 'lexicon 1 percentage in opinion_lexicon.negative():',calculate_percentage(negative_words,negative_list1)\n",
    "\n",
    "print 'lexicon 2 percentage in opinion_lexicon.positive():',calculate_percentage(positive_words, positive_list2)\n",
    "print 'lexicon 2 percentage in opinion_lexicon.negative():',calculate_percentage(negative_words, negative_list2)\n",
    "\n",
    "print 'lexicon 3 percentage in opinion_lexicon.positive():',calculate_percentage(positive_words, positive_list3)\n",
    "print 'lexicon 3 percentage in opinion_lexicon.negative():',calculate_percentage(negative_words, negative_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4c379c7fd57895c160f0dce018da0db65fceef7f344a669a01c9cabe"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "signature": "ebef05138e316018347a5747092dc11105a230d38770462954d39a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of lexicon\n",
      "Hu and Liu lexicon: 0.453799890651\n",
      "lexicon 1: 0.41498086386\n",
      "lexicon 2: 0.452706396938\n",
      "lexicon 3: 0.363039912521\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "my_polarity takes a list and two sets as input, the list represents a single tweet and pset is \n",
    "positive word set and nset the negative word set. If there are more words in pset, return 1. If\n",
    "there are more words in nset, return -1.\n",
    "'''\n",
    "def my_polarity(tweet,pset,nset,):\n",
    "    score=0\n",
    "    for word in tweet:\n",
    "        if word in pset:\n",
    "            score+=1\n",
    "        elif word in nset:\n",
    "            score-=1\n",
    "    if score>0:\n",
    "        return 1\n",
    "    elif score<0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "'''\n",
    "accuracy_of_lexicon takes a tweets list, a labels list and a positive list, a negative list. It \n",
    "calculates the accuracy of the input lexicon.\n",
    "'''\n",
    "def accuracy_of_lexicon(tweets,labels,plist,nlist):\n",
    "    count=0\n",
    "    pset=set(plist)\n",
    "    nset=set(nlist)\n",
    "\n",
    "    for tweet,label in zip(tweets,labels):\n",
    "        if label==my_polarity(tweet,pset,nset):\n",
    "            count+=1\n",
    "\n",
    "    return float(count)/len(labels)\n",
    "\n",
    "print 'accuracy of lexicon'\n",
    "print 'Hu and Liu lexicon:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_words,negative_words)\n",
    "print 'lexicon 1:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list1,negative_list1)\n",
    "print 'lexicon 2:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list2,negative_list2)\n",
    "print 'lexicon 3:',accuracy_of_lexicon(dev_tweets,dev_labels,positive_list3,negative_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the results above, we can see that the accuracy of different lexicons is somehow related to their relationship with Hu and Liu Lexicon. Lexicon 1 and lexicon 2 have relatively high percentage of words in the manual lexicon, and give better performance than lexicon 3. However, all of lexicons show lower accuracy score than logistic regression classifier. This lexicon method does not utilize the information from the training set at all, which might be its primary drawback.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a5b0e40cf431f6ecb3e0e18d6b2ab9213ddcd1198d1cb6bfa0642dbc"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "signature": "f9bf187f20f9c509f47c8b184038fa011048d11feca82ca91b40a55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.505194095134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import sparse\n",
    "\n",
    "'''\n",
    "new_feature_dic method takes tweets from the output of preprocess_file method, feature_dicts\n",
    "from the output of prepare_data, a set of positive words and a set of negative words as input.\n",
    "The polarity type is calculated from every single tweet, then added to the last column of \n",
    "feature_dicts(sparse matrix). Then the new sparse matrix is returned.\n",
    "'''\n",
    "def new_feature_dic(tweets, feature_dicts, pset,nset):\n",
    "    #print (type(feature_dicts))\n",
    "    feature_dicts=feature_dicts.toarray()\n",
    "    polarity_list=[]\n",
    "    for index, tweet in enumerate(tweets):\n",
    "        polarity=my_polarity(tweet,pset,nset)\n",
    "        if polarity==1:\n",
    "            polarity_list.append(1)\n",
    "        elif polarity==0:\n",
    "            polarity_list.append(0)\n",
    "        else:\n",
    "            polarity_list.append(-1)\n",
    "\n",
    "    #print(feature_dicts.shape,np.asarray(polarity_list).shape)\n",
    "\n",
    "    polarity_list=np.array(polarity_list).reshape(feature_dicts.shape[0], 1);\n",
    "    feature_dicts=np.append(feature_dicts,np.asarray(polarity_list),axis=1)\n",
    "\n",
    "    return sparse.csr_matrix(feature_dicts)\n",
    "# new sparse matrix for training data and development data is calculated\n",
    "new_trn_feature_dicts=new_feature_dic(trn_tweets,trn_feature_dicts,set(positive_words),set(negative_words))\n",
    "new_dev_feature_dicts=new_feature_dic(dev_tweets,dev_feature_dicts,set(positive_words),set(negative_words))\n",
    "#Train the best classifier using the new sparse matrix\n",
    "clf_best=LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "print'accuracy', test(clf_best,new_trn_feature_dicts,trn_labels,new_dev_feature_dicts,dev_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the output above, we can see that the accuracy is actually not improved. As a measure to improve the accuracy, we tried to include the polarity type of each tweet inside the sparse matrix representation. However, from the previous experiments, the accuracy for each lexicon is not high by itself, with the most highest Hu and Liu lexicon reaching to merely 45%. This means that the additional information included is sometimes useful, but sometimes misleading. This could be the reason why the performance is not improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5ffc558321e254f8a6893e5affb98867a893b1ffbab8a813756b6e71"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e140733186799a3e75492cb90d87bc53e639ef62db47c93afffb4cf2"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "signature": "d93d0f3cdc8a2acbb253fe0c200fe8421d8d15a6346c8b239dedafcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'text': u\"@IanWright0 really really good Ian, Skyfall tonight, Jack White Tomorrow & Man U away Saturday, can't be bad. How are you?\", u'id': u'357031508', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, -1, 0.45979042800172226, 0.46297651249168464]\n",
      "   \n",
      "{u'text': u\"@JoshNorris @Rotoworld_Draft I'd be pretty mad if the Packers took Bernard in the 1st just bc, Cooper/Eifert would be better IMO.\", u'id': u'424711087', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 1, 0.43624645015579833, 0.51606936819407634]\n",
      "   \n",
      "{u'text': u'Mark Martin qualifies 3rd at Charlotte. Giddy about racing still says \"When I stop racing I will still compete\" http://t.co/vzdEi8LX', u'id': u'63035415', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.35860520008228652, 0.38956616493813212]\n",
      "   \n",
      "{u'text': u'Krul crocked, just putting it out there Craig Gordon britains 2nd most expensive keeper is without a club? #nufc #SAFC #FTM', u'id': u'220419804', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.33107478562024922, 0.41044350079113828]\n",
      "   \n",
      "{u'text': u\"Finished filling out the FAFSA for this scholarship application. Hopefully it's processed before November 15, 'cause I need it by then.\", u'id': u'19822523', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, 0, 0.39571598987615497, 0.59061853183157809]\n",
      "   \n",
      "{u'text': u\"quite enjoying @GNev2 on MNF - he looks more at home sat down tho - I don't think he knows what to do with his hands when he's stood up\", u'id': u'51755720', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, 0, 0.29290875020641954, 0.44161385138860149]\n",
      "   \n",
      "{u'text': u'Physics tomorrow is going to be a shit show', u'id': u'144358410', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.30114191268240592, 0.38833189605631163]\n",
      "   \n",
      "{u'text': u'When I was a kid, Pope John Paul II was crowned or whatever they call it. I remember wondering whatever happened to Pope John Paul the 1st..', u'id': u'16181537', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.39116039636024269, 0.56357585434783108]\n",
      "   \n",
      "{u'text': u\"it's a sunday, i think in the spirit of douglas adams and feeling like shit i will have a second bath\", u'id': u'14314544', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.43379730225520841, 0.50945487078847929]\n",
      "   \n",
      "{u'text': u\"I'm so jealous of everyone at the Justin Bieber Concert. No worries tho. Ill be seeing Maroon 5 in march. @haleyhjerpe #roadtrippin\", u'id': u'404300552', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.4605360552016099, 0.50382106732672716]\n",
      "   \n",
      "{u'text': u\"Stats are stats, & records can be broke.. but crunch time 4th quarter, Jordan had niggaz SCARED to D up... Bron,KD & Kobe can't say the same\", u'id': u'80458354', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.44668925269364679, 0.5432304676747689]\n",
      "   \n",
      "{u'text': u'@MeeshyHope aha yea it still closed and the truck is still filling... Im in my moms car because were leaving for DC tomorrow and we need gas', u'id': u'296563793', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, -1, 0.36443620463589632, 0.40520549953729834]\n",
      "   \n",
      "{u'text': u\"Idgaf who our girls team playing tomorrow! I'm riding with them! Even if they play Baylor!\", u'id': u'449471487', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.4614998464632159, 0.49359281669839866]\n",
      "   \n",
      "{u'text': u\"Speaking of Kanye, he's a genius. Like he may make stupid choices sometimes, but his way of thinking is amazing.\", u'id': u'376981608', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, -1, 0.49232263043122987, 0.49763429187943176]\n",
      "   \n",
      "{u'text': u\"@IanWright0 really really good Ian, Skyfall tonight, Jack White Tomorrow & Man U away Saturday, can't be bad. How are you?\", u'id': u'357031508', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, -1, 0.45979042800172226, 0.46297651249168464]\n",
      "   \n",
      "{u'text': u\"@CoachVac heey do you know anything about UVA's fallll fest loll they invited me so im going this sat but i really dont know what it is loll\", u'id': u'66899098', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, -1, 0.39942957109887184, 0.52679622243471202]\n",
      "   \n",
      "{u'text': u'Quick Panch run with Dozer and Ying turns into me getting home at 1230 on Monday. #collegeproblems', u'id': u'420513635', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, -1, 0.38296869353888857, 0.49392501596045235]\n",
      "   \n",
      "{u'text': u\"Pakistan to talk counterterrorism with US, Afghans: Pakistan's foreign minister revealed Thursday that her country would soon hold co...\", u'id': u'182194111', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.37522280845240025, 0.55453087473313201]\n",
      "   \n",
      "{u'text': u'Top pct of rushing plays: SEA 54.70%, HOU 51.84%, WSH 51.07%, BUF 47.92%, SF 47.85%... #Bears are 9th with 47.04%', u'id': u'870560431', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.40826706305984378, 0.57930074769505968]\n",
      "   \n",
      "{u'text': u'In the Los Angeles area on Nov. 30th- December 1st 2012? This is a red carpet event that you do NOT want to miss! http://t.co/s0lP9Y8c', u'id': u'875284478', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, 0, 0.42599838356055708, 0.55389954566181032]\n",
      "   \n",
      "{u'text': u\"@andrewsikora The Tigers know it's a big game as well: resting Martinez & Alburquerque Monday and bumping Verlander.\", u'id': u'45143149', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, 0, 0.43519825584723165, 0.52899488351774404]\n",
      "   \n",
      "{u'text': u\"@TheyCallMeT_ He's back Saturday but in the PM. I can't promise a kid-free skype but avery usually is better behaved when it's just me\", u'id': u'15136066', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.29707688765204665, 0.47843736762214423]\n",
      "   \n",
      "{u'text': u'Sky picture of the day! #sky #clouds #nature #outside #sun #day #nofilter #instagood http://t.co/8YmeXv5Z', u'id': u'75459354', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.43880616828236763, 0.4434472064410539]\n",
      "   \n",
      "{u'text': u\"@killacath Your Xbox controller broke? What's wrong with it? Tell us more...we may be able to be of assistance! ^LB\", u'id': u'59804598', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, -1, 0.42790908757421925, 0.54540422174381065]\n",
      "   \n",
      "{u'text': u'More NBA tomorrow.. Free NBA league pass till the 6th.. What did I do to deserve this (rozay voice)', u'id': u'35893160', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.44791735557591877, 0.52048506380905968]\n",
      "   \n",
      "{u'text': u'@mtracey such a foolish thought. Santorum and Palin would start a primary challenge in March. Depends on Congress some but EO first day.', u'id': u'11866582', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.27558469438719935, 0.46939357368375295]\n",
      "   \n",
      "{u'text': u\"@bagraider Please tell me you're coming over tomorrow morning so I can make you David Bowie.\", u'id': u'48813357', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.36420401215948289, 0.5592436915020933]\n",
      "   \n",
      "{u'text': u'That 2nd half was shit LA didnt deserve to win it Vancouver played tough and didnt deserve to be screwed by the ref', u'id': u'386339767', u'label': -1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[-1, 0, 0.40130238663902057, 0.5698048935573764]\n",
      "   \n",
      "{u'text': u'Morning Twitterites! Goal: By 5pm Friday, want to see MSM report on Benghazi, and Obama below 42% in polls. HAMMER IT HOME, PATRIOTS!', u'id': u'276194903', u'label': 1}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[1, 0, 0.41234376481903784, 0.44160316996504695]\n",
      "   \n",
      "{u'text': u\"Hoya, Sungyeol and sungjong with Bae yong jun. look..sungyeol's hand! http://t.co/0qXMyy9J\", u'id': u'61709676', u'label': 0}\n",
      "actual class, predicted class, probability for actual class, probability for prediction\n",
      "[0, 1, 0.42288473213396838, 0.44016963833406825]\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# get the classifier with best performance\n",
    "clf_err= LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "\n",
    "'''\n",
    "get_err_list takes the trn_feature_dicts(a sparse matrix), trn_labels, and clf_err(a logistic\n",
    "regression classifier) as input, and returns a list of mis-predicted tweets,its actual label, \n",
    "predicted label, the probability for its actual label, and the probability for its predicted \n",
    "label.\n",
    "\n",
    "The classifier is trained on 4/5 of the training data, and tested on the rest 1/5 data.\n",
    "'''\n",
    "def get_err_list(trn_feature_dicts,trn_labels,clf_err):\n",
    "    len_trn=trn_feature_dicts.shape[0]\n",
    "    clf_err.fit(trn_feature_dicts.toarray()[0:len_trn*4/5],trn_labels[0:len_trn*4/5])\n",
    "    predictions=clf_err.predict(trn_feature_dicts[len_trn*4/5:len_trn])\n",
    "    prediction_probs=clf_err.predict_proba(trn_feature_dicts[len_trn*4/5:len_trn])\n",
    "    i=0\n",
    "    err_list=[]\n",
    "    # pre is predicted label, label is the actual correct label\n",
    "    for pre,label in zip(predictions,trn_labels[len_trn*4/5:len_trn]):\n",
    "        if pre!=label:\n",
    "            probs = prediction_probs[i]\n",
    "            # the probability for the predicted label and the probability for the actual label\n",
    "            predict_prob=0\n",
    "            actual_prob=0\n",
    "            for index,class_label in enumerate(clf_err.classes_):\n",
    "                if class_label==label:\n",
    "                    actual_prob=probs[index]\n",
    "                elif class_label==pre:\n",
    "                    predict_prob=probs[index]\n",
    "            # If the two probabilities are fairly close\n",
    "\n",
    "            if predict_prob-actual_prob<0.2:\n",
    "                err_list.append([original_tweet[len_trn*4/5+i],label,pre,actual_prob,predict_prob])              \n",
    "        i+=1\n",
    "    return err_list\n",
    "\n",
    "err_list=get_err_list(trn_feature_dicts,trn_labels,clf_err)\n",
    "random_index=np.random.randint(len(err_list), size=30)\n",
    "for an_index in random_index:\n",
    "    print err_list[an_index][0]\n",
    "    print 'actual class, predicted class, probability for actual class, probability for prediction'\n",
    "    print err_list[an_index][1:5]\n",
    "    print \"   \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "From the above output, we can see that it is likely that neutral tweets are classified as negative or positive. It is reasonable to suppose that neutral classes is more difficult to classify, as positive and negative classes often have obvious positive words and negative words, while words in the neutral class are likely to be misclassified, thus resulting the whole tweet to be mis-classified. We can consider combining the results from different classifiers to improve the overall accuracy. In the following code, I combined the results from decision tree classifier, logistic regression classifier, and the best lexicon from above, chose the class label that most classifiers predict. When all the classifiers give different results, we will choose the classifier with the best previous performance--logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "03077b4b348fb16cab9f35475a99a16df22012d2107d4592d7b9a7d3"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "signature": "ab77e3b674fbb64ad43b2f03bdcc66cd9cffed98de849fd115a44072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of decision tree 0.483870967742\n",
      "Accuracy of logistic regression 0.503553854565\n",
      "Accuracy of lexicon 0.453799890651\n",
      "Accuracy of ensemble classifier:  0.498086386003\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Improvement by using 3 classifiers together, namely, logistic regression classifier, decision\n",
    "tree classifier, and lexicon classifier. If two of the classifiers give the same result, we \n",
    "will make that the final label. If three classifiers all give different results, we will choose\n",
    "the result of logistic regression as the final result.\n",
    "\n",
    "'''\n",
    "#decision tree\n",
    "clf_decision_tree=DecisionTreeClassifier(splitter='best',min_samples_split=10002)\n",
    "clf_decision_tree.fit(trn_feature_dicts,trn_labels)\n",
    "predictions_decision_tree = clf_decision_tree.predict(dev_feature_dicts)\n",
    "print 'Accuracy of decision tree', accuracy_score(dev_labels,predictions_decision_tree)\n",
    "#logistic regression\n",
    "\n",
    "clf_logistic_regression=LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "clf_logistic_regression.fit(trn_feature_dicts,trn_labels)\n",
    "predictions_logistic_regression=clf_logistic_regression.predict(dev_feature_dicts)\n",
    "print 'Accuracy of logistic regression', accuracy_score(dev_labels,predictions_logistic_regression)\n",
    "#lexicon words\n",
    "lexicon_predictions=[]\n",
    "pset=set(positive_words)\n",
    "nset=set(negative_words)\n",
    "for tweet in dev_tweets:\n",
    "    lexicon_predictions.append(my_polarity(tweet,pset,nset))\n",
    "\n",
    "print'Accuracy of lexicon', accuracy_score(dev_labels,lexicon_predictions)\n",
    "\n",
    "final_predictions=[]\n",
    "for index in range(len(lexicon_predictions)):\n",
    "    dic={}\n",
    "    dic[lexicon_predictions[index]]=dic.get(lexicon_predictions[index],0)+1\n",
    "    dic[predictions_decision_tree[index]]=dic.get(predictions_decision_tree[index],0)+1\n",
    "    dic[predictions_logistic_regression[index]]=dic.get(predictions_logistic_regression[index],0)+1\n",
    "    if dic.get(-1)==2:\n",
    "        final_predictions.append(-1)\n",
    "    elif dic.get(0)==2:\n",
    "        final_predictions.append(0)\n",
    "    elif dic.get(1)==2:\n",
    "        final_predictions.append(1)\n",
    "    else:\n",
    "        final_predictions.append(predictions_logistic_regression[index])\n",
    "\n",
    "\n",
    "print 'Accuracy of ensemble classifier: ',accuracy_score(dev_labels,final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>\n",
    "Surprisingly, the overall performance is not improved. One reason of this is that the classifiers are not totally independent from each other. For example, the logistic classifier and decision tree classifier all train on the same training dataset. If the training dataset is biased, both the classifiers would be biased. Another reason is that, when the results from different classifiers are all different, i.e. -1,0,1 respectively, we will choose the result of logistic regression classifier as the final result. Since the logistic regression classifier is itself not accurate, thus in this situation, the final result is not accurate as well.\n",
    "This new classifier can be regarded as an ensemble classifier, however, it is only guaranteed that most ensemble classifiers give results at least similar to the best classifier, but we do not how much better ensemble classifier will be. In our case, the performance of the ensemble classifier is similar to that of logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2b0f4d9e8ab0fe8d28e48b8dba46d891745a82101b77551f31f52ff2"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c56e9bb9790acf94081360a67b83dc79b7a5a96f159c21d2ccf2f9d3"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "signature": "8c26ed656bdee89c90809063e6a99a73f200067917ed00f4de13cb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of best logistic regression classifier:\n",
      "0.505257332595\n",
      "Classification report of logistic regression classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.36      0.34      0.35       290\n",
      "          0       0.42      0.65      0.51       625\n",
      "          1       0.73      0.46      0.56       892\n",
      "\n",
      "avg / total       0.56      0.51      0.51      1807\n",
      "\n",
      "Accuracy with improvement\n",
      "0.509684560044\n",
      "Classification report of logistic regression classifier with improvement:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.37      0.39      0.38       290\n",
      "          0       0.42      0.64      0.51       625\n",
      "          1       0.74      0.46      0.56       892\n",
      "\n",
      "avg / total       0.57      0.51      0.52      1807\n",
      "\n",
      "Accuracy using best lexicon:\n",
      "0.436635307139\n",
      "Classification report using the best lexicon:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.29      0.41      0.34       290\n",
      "          0       0.36      0.38      0.37       625\n",
      "          1       0.60      0.49      0.53       892\n",
      "\n",
      "avg / total       0.46      0.44      0.45      1807\n",
      "\n",
      "Accuracy using ensemble classifier:\n",
      "0.510237963475\n",
      "Classification report using the ensemble classifier:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.38      0.25      0.30       290\n",
      "          0       0.42      0.72      0.53       625\n",
      "          1       0.73      0.45      0.56       892\n",
      "\n",
      "avg / total       0.57      0.51      0.51      1807\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb69d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#get the original trn_feature_dicts\n",
    "trn_feature_dicts=convert_to_feature_dicts(trn_tweets,True,1)\n",
    "\n",
    "#read in test tweets and preprocess them\n",
    "tst_tweets,tst_labels=preprocess_file('test.json')\n",
    "\n",
    "#get the test feature dictionary\n",
    "tst_feature_dicts=convert_to_feature_dicts(tst_tweets,True,0)\n",
    "\n",
    "#turn the feature dictionaries into sparse matrix\n",
    "trn_feature_dicts,tst_feature_dicts=prepare_data(trn_feature_dicts,tst_feature_dicts)\n",
    "\n",
    "##########################################\n",
    "#The best classifier considering all the classifications above\n",
    "clf_final=LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "print 'Accuracy of best logistic regression classifier:'\n",
    "acc1= test(clf_final,trn_feature_dicts,trn_labels,tst_feature_dicts,tst_labels)\n",
    "print acc1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print 'Classification report of logistic regression classifier:'\n",
    "print classification_report(tst_labels,clf_final.predict(tst_feature_dicts))\n",
    "\n",
    "\n",
    "#######################################\n",
    "#The classifier trained with improved sparse matrix, i.e. add polarity type to the last column\n",
    "#of the sparse matrix, this can be considered as another preprocessing method\n",
    "print 'Accuracy with improvement'\n",
    "new_trn_feature_dicts=new_feature_dic(trn_tweets,trn_feature_dicts,set(positive_words),set(negative_words))\n",
    "new_tst_feature_dicts=new_feature_dic(tst_tweets,tst_feature_dicts,set(positive_words),set(negative_words))\n",
    "#Train the best classifier using the new sparse matrix\n",
    "clf_final2=LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "acc2= test(clf_final2,new_trn_feature_dicts,trn_labels,new_tst_feature_dicts,tst_labels)\n",
    "print acc2\n",
    "print 'Classification report of logistic regression classifier with improvement:'\n",
    "print classification_report(tst_labels,clf_final2.predict(new_tst_feature_dicts))\n",
    "\n",
    "\n",
    "###################################\n",
    "#Using the best lexicon upon the test dataset\n",
    "print 'Accuracy using best lexicon:'\n",
    "acc3= accuracy_of_lexicon(tst_tweets,tst_labels,positive_words,negative_words)\n",
    "print acc3\n",
    "polarity_list1=[]\n",
    "pset=set(positive_words)\n",
    "nset=set(negative_words)\n",
    "for tweet in tst_tweets:\n",
    "    polarity_list1.append(my_polarity(tweet,pset,nset))\n",
    "print 'Classification report using the best lexicon:'\n",
    "print classification_report(tst_labels,polarity_list1)\n",
    "\n",
    "#####################################\n",
    "#ensemble classifier\n",
    "#decision tree\n",
    "clf_decision_tree=DecisionTreeClassifier(splitter='best',min_samples_split=10002)\n",
    "clf_decision_tree.fit(trn_feature_dicts,trn_labels)\n",
    "predictions_decision_tree = clf_decision_tree.predict(tst_feature_dicts)\n",
    "#print 'decision tree', accuracy_score(tst_labels,predictions_decision_tree)\n",
    "#logistic regression\n",
    "\n",
    "clf_logistic_regression=LogisticRegression(solver='lbfgs', max_iter=61)\n",
    "clf_logistic_regression.fit(trn_feature_dicts,trn_labels)\n",
    "predictions_logistic_regression=clf_logistic_regression.predict(tst_feature_dicts)\n",
    "#print 'logistic regression', accuracy_score(dev_labels,predictions_logistic_regression)\n",
    "#lexicon words\n",
    "lexicon_predictions=[]\n",
    "pset=set(positive_words)\n",
    "nset=set(negative_words)\n",
    "for tweet in tst_tweets:\n",
    "    lexicon_predictions.append(my_polarity(tweet,pset,nset))\n",
    "\n",
    "#print'lexicon', accuracy_score(dev_labels,lexicon_predictions)\n",
    "\n",
    "final_predictions=[]\n",
    "for index in range(len(lexicon_predictions)):\n",
    "    dic={}\n",
    "    dic[lexicon_predictions[index]]=dic.get(lexicon_predictions[index],0)+1\n",
    "    dic[predictions_decision_tree[index]]=dic.get(predictions_decision_tree[index],0)+1\n",
    "    dic[predictions_logistic_regression[index]]=dic.get(predictions_logistic_regression[index],0)+1\n",
    "    if dic.get(-1)==2:\n",
    "        final_predictions.append(-1)\n",
    "    elif dic.get(0)==2:\n",
    "        final_predictions.append(0)\n",
    "    elif dic.get(1)==2:\n",
    "        final_predictions.append(1)\n",
    "    else:\n",
    "        final_predictions.append(predictions_logistic_regression[index])\n",
    "\n",
    "print 'Accuracy using ensemble classifier:'\n",
    "acc4= accuracy_score(tst_labels,final_predictions)\n",
    "print acc4\n",
    "print 'Classification report using the ensemble classifier:'\n",
    "print classification_report(tst_labels,final_predictions)\n",
    "\n",
    "\n",
    "#Plot the accuracy of different classification methods.\n",
    "acc_list=(acc1,acc2,acc3,acc4)\n",
    "index=np.arange(4)\n",
    "bar_width = 0.35\n",
    "opacity = 0.4\n",
    "rects1 = plt.bar(index+bar_width/2, acc_list, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 label='Accuracy')\n",
    "plt.xlabel('Prediciton method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of different prediction methods')\n",
    "plt.xticks(index + bar_width, ('logisticRegression', 'LRwithImprovement', 'lexicon','ensembleClassifier'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Final discussion</b>\n",
    "As we can see from above, our classifiers produce similar results on the test dataset to that of the development dataset, and this is what we would usually expect. Through out the assignment, logistic regression classifier gives the overall best accuracy score, and the parameters of this classifier does not seem to affect its performance, so in this final phase, we just initialized a classifier without modifying the default parameters. When the improved sparse matrix(new_trn_feature_dicts,new_tst_feature_dicts) is input to the best classifier, the performance is only slightly improved, and the reason of this has already been analyzed before. \n",
    "\n",
    "The lexicon prediction accuracy is always worse than logistic regression. One drawback is the lexicon itself is not perfect. Lexicons are generated manually or automatically. When people do the indexing for each word in the lexicon, they will unavoidably get affected by their own beliefs or emotions, thus resulting in a subjective lexicon. On the other hand, however, when the lexicon is generate automatically using seed words, modelling their relationship using cosine distance is not accurate, more importantly, the vectors used to represent these words are not accurate, thus resulting in a lexicon of low quality. Only the manually generated lexicon and the second lexicon are discussed here because the other lexicons show even worse performance.\n",
    "\n",
    "Another interesting fact noteworthy is that the precision score of positive tweet is much higher than that of neutral or negative tweet. This means that when a tweet is predicted as positive, it is very likely to be positive. By further looking at the support column, the reason becomes obvious. Positive tweet constitutes half of all the tweets, so the baseline of accuracy for predicting a tweet as positive is nearly 50%. The same reason also accounts for the relatively low precision score for class 0 and -1. \n",
    "\n",
    "All of the accuracy scores above are somewhere near 50%, which is relatively low, indicating that the task is not an easy one. A possible measure of improvement could be to refine the preprocessing part, as discussed before, to make the hashtag analyzing part more intelligent in recognizing different words. Moreover, hashtag itself might be a good representation of the whole tweet, so it is reasonable that hashtags be given more weight when calculating polarity scores. \n",
    "\n",
    "In addition, when calculating the polarity scores, all the words are given the same weight. This is not correct. Some words are stronger than others, for example, excellent may be equal to very good, but is stronger than good. Therefore, assigning the same weight to all the words omits the difference between words, and is certainly not accurate.\n",
    "\n",
    "Besides the machine learning algorithms we have used, we may also try other algorithms such as support vector machine and neural network. No doubt that some algorithms are more suitable for some problems than others, which is also proved by our results—logistic regression classifier is always giving better performance than decision trees. So it is always good that we have more time to try and learn about different algorithms and different parameter combinations.\n",
    "\n",
    "Finally, the original classes of tweets are also questionable. Are these classes labeled by human beings? People have various opinions regarding the same sentence,  and positive tweets in someone’s eyes could be regarded as negative by others. Even the original training labels are doubtful, it is unlikely that we will build a good classifier out of it. In my opinion, sentiment analysis of text is not an objective task to some level, and how it can be made more scientific and precise requires further investigation. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
