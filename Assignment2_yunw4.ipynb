{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0c217e712496103b2c70c746416cd40ff12c6846c8c1ebebe604126f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP90042 Assignment #2: Cross-language Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "f8854a7d98e8bb9f29c327482f16944ac2fecedfdd13b71fe4923506"
   },
   "source": [
    "```Student Name: Yun Wang\n",
    "Student ID: 672323```\n",
    "\n",
    "Please do not edit this field. It must be left as is for use in marking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "98dcec95a07a7a3c57b81090e6baa53ebba4f2e05353119d77c99f0e"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2439330c1b7df349798216345053a8c4c40c892fa80637d5fec5afda"
   },
   "source": [
    "**Due date**: 5pm, Wed 25th May\n",
    "\n",
    "**Submission method**: see LMS\n",
    "\n",
    "**Submission materials**: completed copy of this ipython notebook. *Do not upload the corpora*, you can assume these files will be extracted into the same folder as the code.  *Do not use absolute paths* such as paths to files on your machine, use *relative paths* instead. *Include output* in the ipython notebook, and ensure your code can run sucessfully from scratch (e.g., use *Kernel -> Restart and Run All* menu option to test.)\n",
    "\n",
    "**Late submissions**: -10% per day, no late submissions after the first week\n",
    "\n",
    "**Marks**: 25% of mark for class\n",
    "\n",
    "**Overview**: For this project, you'll be building a cross language information retreival system (CLIR), which is capable of searching for important text documents written in a English given a query in German, and displaying the results in German. This incorporates machine translation, information retrieval using a vector space model, and IR evaluation. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "**Materials**: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages as used for workshops and the previous assignments. The main part of this assignment should be done entirely in python, using built-in python packages and the packages used in the class (NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim). Please do no use any other 3rd party packages. Note that some choices of the *extended part* of the assignment may require you to work with other tools, such as 3rd party translation tools or retreival engines, in which case this part of the assignment will require working in another environment, although you should report your results in the ipython notebook and attach your other files (code, shell scripts etc) with your submission.   \n",
    "\n",
    "You are encouraged to make use of the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. Adapting code from the class notebooks is permitted.\n",
    "\n",
    "**Data**: The other data you will need are:\n",
    "  - parallel German-English corpus.  This data comes from the Europarl corpus, which is a collection of debates held in the EU parliament over many years, translated into several EU languages. This data was collated as part of the annual WMT translation competitions. The data has been tokenised, sentence aligned using the Church-Gale algorithm. \n",
    "  - CLIR evaluation corpus, comprising a large collection of English documents sourced from Wikipedia along with several German query strings and relevance judgements for each document and query pair. \n",
    "  \n",
    "The data files are as follows:\n",
    "  - *bitext-small.{en,de}* is the sentence-aligned parallel corpus, of a small enough size for you to use for developing word-alignment tools. You may wish to work on a smaller subset during code development.\n",
    "  - *bitext-large.{en,de}* is a much larger version of the above corpus, for language modelling and translation.\n",
    "  - *newstest2013.{en,de}* is a separate small parallel corpus reserved for evaluation.\n",
    "  - *dev.{docs,queries,qrel}* is a set of documents in English, queries in German and query relevance judgements, for the development of the IR components and evaluation.\n",
    "  \n",
    "For details of the datasets, please see:\n",
    "> Philipp Koehn. *Europarl: A Parallel Corpus for Statistical Machine Translation* MT Summit 2005.   \n",
    "\n",
    "and\n",
    "> Shigehiko Schamoni, Felix Hieber, Artem Sokolov, Stefan Riezler. *Learning Translational and Knowledge-based Similarities from Relevance Rankings for Cross-Language Retrieval* ACL 2014.\n",
    "\n",
    "You can find the PDFs for both papers online with a quick web search.\n",
    "\n",
    "**Evaluation**: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 15 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. Indexing or translation steps might taken longer than this, however you should upload with your submission preprocessed files (e.g., text files, pickled python structures) to keep the runtime modest.\n",
    "You will be marked not only on the correctness of your methods, but also on your explanation and analysis. \n",
    "\n",
    "Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "**Updates**: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "**Academic Misconduct**: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s [Academic Misconduct policy](http://academichonesty.unimelb.edu.au/policy.html) where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "64ecc63dfd9ac35e3cc8d65cfa7f1941bf053155396a80a716f2971a"
   },
   "source": [
    "### Warning: File encodings and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "94f7a362e73cbab206a1df7f6c7d7fb81979e21ab06a096e29aa93b6"
   },
   "source": [
    "All the text files are encoded in *utf-8*, which requires some care in using with python strings, the nltk tools and jupyter.  Please use the following method to convert strings into ASCII by escaping the special symbols. Be careful to do the conversion after tokenisation, as the escaped tags might interfere with the NLTK tokenizer and get treated as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "signature": "e420788a690b2b073acaff140f6eaef027d13d6c9165c534d2f70f1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seit',\n",
       " 'damals',\n",
       " 'ist',\n",
       " 'er',\n",
       " 'auf',\n",
       " '\\\\xfcber',\n",
       " '10.000',\n",
       " 'punkte',\n",
       " 'gestiegen',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "tokenize(\"Seit damals ist er auf über 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "be042ba99a931a5592beb93bf6eb60464b967a5dddc37fde2934afaf"
   },
   "source": [
    "# Part 1: CLIR engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c0651968b424264054d55e4d8ecbd55a25aae56178ca7c94aeee2245"
   },
   "source": [
    "The project has two parts: first you must build a CLIR engine, comprising information retrieval and translation components, and evaluate its accuracy. Next you will propose an extension to a component of the system. In each step you will need to justify your modelling and implementation decisions, and evaluate the quality of the outputs of the system, and at the end you will need to reflect on the overall outcomes.\n",
    "\n",
    "The CLIR system will involve:\n",
    " - translating queries from German into English, the language of our text collection, using word-based translation\n",
    " - once the queries and the documents are in the same language, search over the document collection using BM25\n",
    " - evaluate the quality of ranked retreival results using the query relevance judgements\n",
    "\n",
    "Note that you could try translating the text collection into German instead, but for this project we'll stick with translating the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "90bfb15bc9553c1b9b334abd0fb0b11716bd3032e062fe34a10f8a32"
   },
   "source": [
    "Building the CLIR engine is the majority of work in the assignment, and  constitutes 70% of the assignment mark. Note that although there are several steps, they do not necessarily need to be attempted in a linear order. That is, some components are independent of others. If you're struggling with one component and cannot complete it, then you may be able to skip over it and return to it later. Where outputs are needed in a subsequent step you may want to consider ways of side-stepping this need, e.g., by using Google translate output rather than your system output for translating the queries. You should aim to answer all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "20a229917248e30bcf551f4f2d98af34cedc61d5be5a9fc64f83a7fd"
   },
   "source": [
    "## Information Retreival with BM25 (20% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "d83bd824ebc8910599e3dd8b4054075c8e15db63907d97507fbd0b39"
   },
   "source": [
    "You should implement a vector-space retreival system using the BM25 approach. You'll need to:\n",
    " - load in the data files and tokenize the input,\n",
    " - preprocess the lexicon, e.g., by stemming and stop-word removal, \n",
    " - calculate the TF/IDF representation for all documents in the collection,\n",
    " - store an inverted index such that you can efficiently recover the documents matching a query term, and\n",
    " - implement querying in the BM25 model\n",
    " - test that it runs with some English queries (you'll have to make these up)\n",
    " \n",
    "This should run in a reasonable time over *dev.docs* (up to a few minutes to index), but beyond this does not need to be highly optimised. Feel free to use python dictionaries, sets, lists, numpy/scipy matrices etc where appropriate for best runtime, but you shouldn't need to use specialised data structures such as compression schemes or the like. Feel free to use APIs in NLTK, scikit-learn and other allowed python modules as needed (see list above.) You will probably want to save and load your index to/from disk to save repeated re-indexing every time you load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "signature": "85b5e445a93f508223134a41012c24ba0ef23fa57e652589c22e8e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdf,df,doc_length,original_docs completed\n",
      "query1 completed ['123', 'student', 'protest', 'against', 'too', 'much', 'homework']\n",
      "26194672\tpostcards from the wedge postcards from the wedge is the fourteenth episode of the simpsons twenty first season it originally aired on the fox network in the united states on march 14 2010 in the episode homer and marge once again try to discipline bart after mrs. krabappel tells them that bart has not been doing his homework but bart has a plan to manipulate homer 's strictness and marge 's sympathetic ear which backfires when homer and marge see through the plan and decide to ignore bart these themes had been seeded in the previous season in episodes like double double boy in trouble and the good the sad and the drugly and would culminate in the show 's first ever true grounding and the first to stand for the rest of the episode the episode was written by brian kelley and directed by mark kirkland the episode features references to the shows pokémon house and the jetsons the episode received mostly positive reviews and got an 18-49 nielsen rating of 2.6/8 plotat school after mrs. krabappel shows a video from 1956 to her students about the future she tells her students to turn in their homework project which they had three months to do bart did not even attempt to complete his homework so he tries to make his homework on the fly out of odds and ends found in his desk unfortunately mrs. krabappel does not approve and prepares to send a letter to his parents she gives it to martin to mail and he heads out the door bart shoots an eraser at the pull station breaking the glass pressing the button and activating the alarm everyone evacuates but bart runs through 6th graders and gets through them he almost gets the letter when it drops out of the mail slot but groundskeeper willie heads off with the mailbag homer and marge then receive the letter from edna krabappel informing them that bart is one month behind on his homework when homer is informed that he does not have to help bart with this work he is eager to increase his son 's workload marge however is concerned that the heavy workload will dissuade bart from liking school ignoring or unaware of the fact that he already hates it when bart realizes his parents do not agree on this issue he uses their opposing views to avoid homework entirely lisa explains that this is a wedge issue an issue that sharply divides two parties marge and homer begin to argue more and more with bart inciting the two to argue about very minor things that even do not involve\n",
      "\n",
      "5400718\tstudent protest student protest encompasses a wide range of activities that indicate student dissatisfaction with a given political or academic issue and mobilization to communicate this dissatisfaction to the authorities university or civil or both and society in general and hopefully remedy the problem protest forms include but are not limited to sit ins occupations of university offices or buildings strikes etc. more extreme forms include suicide such as the case of jan palach 's and jan zajíc 's protests against the end of the prague spring and kostas georgakis protest against the greek military junta of 1967 -- 1974 student strikea common tactic of student protest is to go on strike sometimes called a boycott of classes this occurs when students enrolled at a teaching institution such as a school college or university refuse to go to class it is meant to resemble strike action by organized labour it is often used as a negotiating tactic in order to put pressure on the governing body of the university particularly in countries where education is free and the government cannot afford to have a student cohort miss an entire year this can cause an overload of students in one academic term and the absence of an entire class in the following term in the west student strikes date to the early days of universities in the middle ages with one of the earliest and most significant being the university of paris strike of 1229 which lasted 2 years and yielded significant concessions in more recent times significant walkouts occurred in the late 1960s and early 1970s -- the french may 1968 uprisings began as a series of student strikes the largest student strike boycott in american history occurred in may and june 1970 in the aftermath of the american invasion of cambodia and the killings of student protesters at kent state university in ohio an estimated four million students at more than 450 universities colleges and high schools participated in the student strike of 1970 the term student strike has been criticized as inaccurate by some unions and commentators in the news media these groups have indicated that they believe the term boycott is more accurate examples 2012 quebec student protests 2012 valencia student protests 2011\n",
      "\n",
      "203901\tstudent nonviolent coordinating committee the student nonviolent coordinating committee sncc was one of the organizations of the american civil rights movement in the 1960s it emerged from a student meeting organized by ella baker held at shaw university in april 1960 sncc grew into a large organization with many supporters in the north who helped raise funds to support sncc 's work in the south allowing full time sncc workers to have a 10 per week salary many unpaid volunteers also worked with sncc on projects in mississippi alabama georgia arkansas and maryland sncc played a major role in the sit ins and freedom rides a leading role in the 1963 march on washington mississippi freedom summer and the mississippi freedom democratic party over the next few years sncc 's major contribution was in its field work organizing voter registration drives all over the south especially in georgia alabama and mississippi in the later 1960s led by fiery leaders such as stokely carmichael sncc focused on black power and then protesting against the vietnam war as early as 1965 james forman said he did n't know how much longer we can stay nonviolent and in 1969 sncc officially changed its name to the student national coordinating committee to reflect the broadening of its strategies it passed out of existence in the 1970s founding and early years chairmen 's of the student nonviolent coordinating committee marion barry chairmen 1960 -- 1961 charles f. mcdew chairmen 1961 -- 1963 john lewis chairmen 1963 -- 1966 stokely carmichael chairmen 1966 -- 1967 h. rap brown chairmen 1967 -- 1968 founded in 1960 and inspired by the greensboro and nashville sit ins independent student led groups began direct action protests against segregation in dozens of southern communities the most common action of these groups was organizing sit ins at racially segregated lunch counters to protest the pervasiveness of jim crow and other forms of racism in addition to sitting in at lunch\n",
      "\n",
      "query2 completed ['123', 'where', 'is', 'the', 'best', 'japanese', 'restaurant']\n",
      "6002567\tsts - 123 sts 123 was a space shuttle mission to the international space station iss which was flown by space shuttle endeavour sts 123 was the 1j a iss assembly mission the original launch target date was 14 february 2008 but after the delay of sts 122 the shuttle was launched on 11 march 2008 it was the twenty fifth shuttle mission to visit the iss and delivered the first module of the japanese laboratory japanese experiment module kibō and the canadian special purpose dexterous manipulator spdm dextre robotics system to the station the mission duration was 16 days and 14 hours and it was the first mission to fully utilize the station to shuttle power transfer system sspts allowing space station power to augment the shuttle power systems the mission set a record for a shuttle 's longest stay at the iss crew mission payloads location cargo mass bay 1 -- 2 orbiter docking system emu 3003 emu 3004 bay 3p shuttle power distribution unit spdu bay 3s canadarm2 yaw joint bay 4p misse pec 6a bay 4s direct current switching unit dcsu bay 5p misse pec 6b bay 5s direct current switching unit dcsu bay 6s standard interface panels bay 7 -- 8 dextre on spacelab pallet bay 9p ecsh eva cargo stowage on apc bay 10 -- 12 kibo elm ps bay 11s standard interface panels bay 13p lightweight adapter plane for misse bay 13s usaf rigex experiment starboard sill orbiter boom sensor system port sill canadarm total sts 123 delivered the pressurized section of the japanese experiment logistics\n",
      "\n",
      "3210335\tunagi unagi う な ぎ is the japanese word for freshwater eel especially the japanese eel anguilla japonica unagi is a common ingredient in japanese cooking it is not to be confused with saltwater eel which is known as anago in japanese in japanese cuisineunagi is served as part of unadon sometimes spelled unagidon especially in menus in japanese restaurants in western countries a donburi dish with sliced eel served on a bed of rice a kind of sweet biscuit called unagi pie made with powdered unagi also exists unagi is high in protein vitamin a and calcium specialist unagi restaurants are common in japan and commonly have signs showing the word unagi with hiragana う transliterated u which is the first letter of the word unagi lake hamana in hamamatsu city shizuoka prefecture is considered to be the home of the highest quality unagi as a result the lake is surrounded by many small restaurants specializing in various unagi dishes unagi is often eaten during the hot summers in japan there is even a special day for eating unagi the midsummer day of the ox doyo no ushi no hi unakyu is a common expression used for sushi containing eel cucumber due to the health hazards of eating raw freshwater fish eels are always cooked and in japanese food are always served with tare sauce sustainabilityseafood watch a sustainable seafood advisory list recommends that consumers avoid eating unagi due to significant pressures on worldwide freshwater eel populations all three eel species used as unagi have seen their population sizes greatly reduced in the past half century for example catches of the european eel have declined about 80 since the 1960s although about 90 of freshwater eel consumed in the u.s. are farm raised they are not bred in captivity instead young eels are collected from the wild and then raised in various enclosures in addition to wild eel populations being reduced by this process eels are often farmed in open net pens which allow parasites waste products and diseases to flow directly back into wild eel habitat further threatening wild populations freshwater eels\n",
      "\n",
      "6503128\tiodine - 123 iodine 123 123 i or i 123 is a radioactive isotope of iodine used in nuclear medicine imaging including single photon emission computed tomography spect the isotope 's half life is 13.22 hours the decay by electron capture to tellurium 123 emits gamma radiation with a predominant energy of 159 kev this is the gamma primarily used for imaging in medical applications the radiation is detected by a gamma camera the isotope is typically applied as iodide 123 the anionic form production iodine 123 is produced in a cyclotron by proton irradiation of xenon in a capsule xenon 124 absorbs a proton and immediately loses a neutron and proton to form xenon 123 or else loses two neutrons to form caesium 123 which decays to xenon 123 the xenon 123 formed by either route then decays to iodine 123 and is collected on the side of the capsule under refrigeration then eluted with dilute sodium hydroxide in a halogen disproportionation reaction similar to collection of iodine 125 after it is formed from xenon by neutron irradiation see that article for more xe 124 p -> i 123 2n 0 2e -1 iodine 123 is usually supplied as the iodide and hypoiodate in dilute sodium hydoxide solution at high isotopic purity i 123 for medical applications has also been produced at oak ridge national laboratories by proton cyclotron bombardment of 80 isotopically enriched tellurium 123 decay the detailed decay mechanism is electron capture to form an excited state of the nearly stable nuclide tellurium 123 half life so long that it is considered stable for all practical purposes this excited state of te 123 produced is not the metastable nuclear isomer te 123m the decay of i 123 does not involve enough energy to produce te 123m but rather is a lower energy nuclear isomer of te 123 that immediately gamma decays to ground state te 123 at the energies noted or else 13 of the time decays by internal conversion electron emission\n",
      "\n",
      "query3 completed ['123', 'school', 'teacher', 'student', 'education']\n",
      "5325141\teducational psychologist an educational psychologist is a psychologist whose differentiating functions may include diagnostic and psycho educational assessment psychological counseling in educational communities students teachers parents and academic authorities community type psycho educational intervention and mediation coordination and referral to other professionals at all levels of the educational system many countries use this term to signify those who provide services to students their teachers and families while other countries use this term to signify academic training in the discipline of educational psychology with no intention of preparing persons to provide services specific factspsychology is a so well developed discipline that allows different specializations a clinical and health psychology b work and organizational psychology c educational psychology etc. what differentiates an educational psychologist from other psychologists or specialists is constituted by an academic triangle whose vertexes are represented by three categories teachers students and curricula see diagram the use of plural in these three cases assumes two meanings a the traditional or official one and b other more general derived from our information and knowledge society the plural also indicates that nowadays we can no longer consider the average student or teacher or a closed curriculum but the enormous variety found in our students teachers and curricula the triangle vertexes are connected by two directional arrows allowing four fold typologies instead of the traditional two way relationships e.g. teacher student in this way we can find in different educational contexts groups of good teachers and students excellent teaching learning processes and products groups of good teachers but bad students and groups of bad teachers and good students producing in both cases lower levels of academic achievements in addition we can find groups of bad teachers and bad students school failure this specific work of an educational psychologist takes place in different contexts micro meso and macro systems microsystems refer to family contexts where atmosphere hidden curriculum and expectations and behaviors of all family members determine to a large extent the educational development of each student the term mesosystem refers to all variety of contexts found in educational institutions knowing that different variables such as geographical location institution marketing or type of teachers\n",
      "\n",
      "5509978\tvillage institutes village institutes in turkish köy enstitüleri is a group of co ed public boarding normal schools that were operational between 1940 and 1954 in turkey they were the cornerstones of the rural development project at the time there were n't any schools in most of the villages village institutes are established to train teachers for each village and send them back to form new village schools despite their short life they highly increased the number of primary schools in the country they had strong support from the prime minister i̇smet i̇nönü and mostly established by minister of education hasan ali yücel and the head of primary education i̇smail hakkı tonguç students were selected among the most successful students in the villages in most of the institutes students built all the school buildings and farmed their own food their education included both practical agriculture construction arts and crafts etc. and classical mathematics science literature history etc. courses their daily routine included morning gymnastics reading hours and farming they also had weekly meetings in which students can freely criticize teachers and school administration in the end of their service there were a total of twenty village institutes and one superior village institute that trains teachers for the others they gave about 25000 graduates despite their great benefits many parts of the society were against these schools conservatives opposed the co ed education in a boarding school it was very hard to persuade parents at villages to let their daughters to study there anti communist and anti socialist movements strong at the time attacked the schools and lower their reputation in the society school libraries contained leftist books as well and students were expected to read different political thoughts also many landlords that control villages are disturbed by the highly educated teachers coming back they not only formed primary schools but also educated the villagers both intellectually and about agriculture in 1945 the village institutes began to be subjected to violent attacks by the conservative wing of the rpp and the newly founded dp the village institutes were accused of fostering an subversive unruly anti traditional generation and being the hotbeds of marxist indoctrination these attacks were waged mainly by the great\n",
      "\n",
      "40895576\tbangkapi school bangkapi school which is situated about 15 miles from the center of bangkok in a village called klongchan in bangkok suburbs is a fairly large co educational high school in about 49 rai of land nearly 20 acres of land historythe school was founded by the ministry of education on march 25 1955 as a small secondary school consisting of one wooden building 81 students of m .1 level grade 7 2 classrooms and 3 teachers the first official teaching day was on may 17 1955 the initial development was very slow the addition of two wooden buildings and usom supplying the school with some instructional aids typewriters and other devices occurred in 1957 mr. sawat phonpoke was the first principal appointed to work here nowadays the school consists of more than 120 professional teachers in charge of diverse tasks as appropriately designated in the school there are 5 large teaching blocks 2 multi purposed buildings 2 workshops an old thai style house a standard swimming pool a football pitch a volleyball court 3 basketball courts and approximately 3,200 students who fall into 72 classrooms from m .1 grade 7 to m .6 grade 12 this makes the school one of the extra large secondary schools in bangkok the syllabi used in the school are provided to make education available to the large number of students who desire training and preparation for many diverse vocational goals and branches of knowledge according to the school national curriculum that emphasizes on basic needs and different individuals at all levels of the students it is a must for the junior and senior high school students to earn a minimum of 80 and 75 units of study relatively to meet the requirement for their graduation to fulfill this goal the school has added over the years many varied courses to suit the needs of the growing metropolis giving the students education requires more than merely teaching them knowledge it requires the right way to help them and bring them up to attain their potential in both challenging and demanding the task needs patience dedication and vision it is therefore essential that parents and teachers work together to avoid a mismatch of values that can lead to emotional\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.translate.phrase_based import phrase_extraction\n",
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import *\n",
    "from nltk.translate import IBMModel1\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "import math\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_word_set=set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "'''\n",
    "preprocess_file method\n",
    "input\n",
    "filename:the name of document to be indexed\n",
    "\n",
    "output\n",
    "tdf: a dictionary of term frequency in each document, {term1:{document1:3,document2:2,...},term2:{document2:4,document5:3},...}\n",
    "df: document frequency dictionary  {term1:5,term2:6,...}\n",
    "doc_length: document length dictionary  {document1:213, document2:198,...}\n",
    "original_docs: a dictionary which contains the original documents\n",
    "\n",
    "The data file is loaded and tokenized, then stop words are removed and words are stemmed. The output variables are\n",
    "calculated.\n",
    "'''\n",
    "def preprocess_file(filename):\n",
    "    #dtf={}\n",
    "    #term document frequency\n",
    "    tdf={}\n",
    "    #document frequency\n",
    "    df={}\n",
    "    # document length\n",
    "    doc_length={}\n",
    "    # a dictionary of original documents. This is stored to retrieve the actual query results\n",
    "    original_docs={}\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            processed_words={}\n",
    "            #tokenize each sentence\n",
    "            aWordList=tokenize(line)\n",
    "            len=0\n",
    "            for i,word in enumerate(aWordList):\n",
    "\n",
    "                if i !=0:\n",
    "                    if word not in stop_word_set:\n",
    "                        stemmed=stemmer.stem(word)\n",
    "                        len+=1\n",
    "                        if stemmed not in tdf:\n",
    "                            tdf[stemmed]={}\n",
    "                            tdf[stemmed][aWordList[0]]=1\n",
    "                        else:\n",
    "                            tdf[stemmed][aWordList[0]]=tdf[stemmed].get(aWordList[0],0)+1\n",
    "\n",
    "\n",
    "                        processed_words[stemmed]=processed_words.get(stemmed,0)+1\n",
    "            original_docs[aWordList[0]]=line\n",
    "            doc_length[aWordList[0]]=len\n",
    "            for word in processed_words:\n",
    "                df[word]=df.get(word,0)+1\n",
    "\n",
    "\n",
    "            #dtf[aWordList[0]]=processed_words\n",
    "\n",
    "    return tdf,df,doc_length,original_docs\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "tokenize method(same as provided in the assignment)\n",
    "input\n",
    "line: a string to be tokenized\n",
    "output\n",
    "a list of words\n",
    "'''\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "bm25 method\n",
    "input\n",
    "tdf: a dictionary of term frequency in each document\n",
    "df: document frequency dictionary\n",
    "doc_length: document length dictionary\n",
    "num_doc: number of the documents\n",
    "doc_length_ave: the average length of documents\n",
    "query: a list of words\n",
    "k1,b,k3: parameters of BM25 model\n",
    "\n",
    "output\n",
    "results: a list of documents sorted according to their scores\n",
    "\n",
    "This method implements the BM25 model and returns a list of sorted documents.\n",
    "'''\n",
    "def bm25(tdf,df,doc_length,num_doc,doc_length_ave,query,k1,b,k3):\n",
    "\n",
    "    qt=dict()\n",
    "    # stemming the query\n",
    "    for term in query:\n",
    "        if term not in stop_word_set:\n",
    "            stemmed=stemmer.stem(term)\n",
    "            qt[stemmed]=qt.get(stemmed,0)+1\n",
    "\n",
    "    # a dictionary to hold the BM25 score for each document\n",
    "    results=dict()\n",
    "    for term in qt:\n",
    "        if term in tdf:\n",
    "            for doc in tdf[term]:\n",
    "                ft=df[term]\n",
    "                fdt=tdf[term][doc]\n",
    "                ld=doc_length[doc]\n",
    "                fqt=qt[term]\n",
    "                results[doc]=results.get(doc,0)+math.log((num_doc-ft+0.5)/(ft+0.5))*(k1+1)*\\\n",
    "                                                float(fdt)/(k1*((1-b)+b*ld/doc_length_ave)+fdt)*\\\n",
    "                                                (k3+1)*fqt/float(k3+fqt)\n",
    "\n",
    "    # sort the documents according to their score and store the results into a list\n",
    "    results=sorted(results, key=results.get, reverse=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "tdf,df,doc_length,original_docs=preprocess_file('dev.docs')\n",
    "\n",
    "\n",
    "print 'tdf,df,doc_length,original_docs completed'\n",
    "num_doc=float(len(doc_length))\n",
    "doc_length_ave=sum(doc_length.values())/num_doc\n",
    "\n",
    "'''\n",
    "list_to_save=[tdf,df,doc_length,original_docs,num_doc,doc_length_ave]\n",
    "\n",
    "with open('objs.pickle', 'w') as f:\n",
    "    pickle.dump(list_to_save, f)\n",
    "    \n",
    "    \n",
    "tdf,df,doc_length,original_docs,num_doc,doc_length_ave=[0,0,0,0,0,0]\n",
    "with open('objs.pickle') as f:\n",
    "    tdf,df,doc_length,original_docs,num_doc,doc_length_ave = pickle.load(f)\n",
    "'''\n",
    "#the first number represent query ID, which is used to calculate query relevance\n",
    "# and is useless here\n",
    "query_example=['123','student','protest','against','too','much','homework']\n",
    "#default parameters are chosen for the BM25 model\n",
    "results=bm25(tdf,df,doc_length,num_doc,doc_length_ave,query_example,1.5,0.5,0)\n",
    "\n",
    "print 'query1 completed', query_example\n",
    "\n",
    "for docid in results[0:3]:\n",
    "    print original_docs[docid]\n",
    "    \n",
    "query_example2=['123','where','is','the','best','japanese','restaurant']\n",
    "#default parameters are chosen for the BM25 model\n",
    "results2=bm25(tdf,df,doc_length,num_doc,doc_length_ave,query_example2,1.5,0.5,0)\n",
    "\n",
    "print 'query2 completed',query_example2\n",
    "\n",
    "for docid in results2[0:3]:\n",
    "    print original_docs[docid]\n",
    "    \n",
    "query_example3=['123','school','teacher','student','education']\n",
    "#default parameters are chosen for the BM25 model\n",
    "results3=bm25(tdf,df,doc_length,num_doc,doc_length_ave,query_example3,1.5,0.5,0)\n",
    "\n",
    "print 'query3 completed',query_example3\n",
    "\n",
    "for docid in results3[0:3]:\n",
    "    print original_docs[docid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ccd0094455c54403ffc015f1bf96323cb3c23f66db7b7e1e7246ffb2"
   },
   "source": [
    "<b> Analysis </b>\n",
    "\n",
    "In the above code, we chose three queries and printed the top 3 results for each query.\n",
    "The first 2 queries are both sentences, while the third query is a bag of words. Because the BM25 model does not care about the order of the word in the query, the model is more suitable to find documents which is related to a certain area, not answering questions. For example, in the third query, where all the words are related to education, the model did a pretty good job at finding the relevant documents. However, in the first two queryies, because BM25 treat the query as a bag of words, we could not find what we are looking for. Suppose we have two documents, both containing all the words from query 2--one contains the words in exactly the same order as query 2, where we are likely to find the correct answer for \"where is the best Japanese restaurant\", the other contains the words in arbitrary order, where it is highly impossible to find the answer for our query. Under BM25 model, both documents would be ranked with the same score. That is to say, BM25 does not consider the order of terms in the query, it only cares about the presence and frequency of the words, which is a major drawback.\n",
    "\n",
    "In addition, in the second query, \"where is best Japanese restaurant\", the word \"where\" is probably a high frequency word in the document collection, therefore, under the BM25 model, its impact on the final result becomes less. However, if we analyze the query by its meaning, this query actually intends to find the place of the best Japanese restaurant, not the price of the restaurant, nor the opening hours of this restaurant. BM25 reduces term importance if it appears in many documents, however, in this case, the term \"where\" carries necessary information for us to solve the query. Although it is useful to reduce the importance of frequent words like \"am\", \"is\", \"are\", \"the\", \"a\",\"what\", \"where\", \"which\", \"when\", it is sometimes very dangerous to do so, and this may be another drawback of the BM25 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "b2e661ca24168084f8b45420b4445111113315974c03693965bd7bae"
   },
   "source": [
    "## Translating the queries (40% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "ec92a09a7ec685d08d181ffbdca9061afc3442223d9fa6f39fa2e366"
   },
   "source": [
    "For translation, you should implement a simple word-based translation model in a noisy channel setting, where you search for the best translation string using a language model over the English with a translation model over the back-translation of the English output string, $\\vec{e}$, into the German input string, $\\vec{f}$. This corresponds to finding the string, $\\vec{e}$ which maximises $p(\\vec{e}) p(\\vec{f} | \\vec{e})$. This has two components:\n",
    "\n",
    "### Language model\n",
    "\n",
    "The first step is to estimate a language model. You should learn both a unigram language model and a trigram language model, and compare the two. Note that the unigram will be used in the *decoding* step below. You will have to think about how to smooth your estimates, and the related problem of handling unknown words. For smoothing the trigram model, you should you use backoff smoothing. There are a few choices to be made about how much probability mass to assign to the backoff, you will need to decide how to do this -- see the lecture slides and readings for some ideas. Be careful to pad the sentence with sentinel symbols to denote the start and end of each sentence such that the model can predict the words at the start of a sentence, and those when a sentence is complete.\n",
    "\n",
    "Please use *bitext-large.en* to train your models (start with *bitext-small.en* for your development) and evaluate the perplexity of your model on *newstest2013.en*. You should justify your development decisions and parameter settings in a markdown cell, and quantify their effect on perplexity, including the differences you notice between the unigram and the trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "signature": "24ace0be6054e2d7634ee5392b0bd73c935e4d195b4ad5481a89770c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_counts, bigram_counts,trigram_counts, token_count completed\n",
      "unigram perplexity,using add k, k= 1,2,3\n",
      "611.82796244\n",
      "614.95546767\n",
      "618.772985829\n",
      "trigram perplexity\n",
      "132.539743873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "check_for_unk_train method takes a word and a dictionary of unigram counts as input, and returns\n",
    "UNK if the word is not in the unigram_counts dictionary\n",
    "'''\n",
    "def check_for_unk_train(word,unigram_counts):\n",
    "    if word in unigram_counts:\n",
    "        return word\n",
    "    else:\n",
    "        unigram_counts[word] = 0\n",
    "        return \"UNK\"\n",
    "\n",
    "\n",
    "'''\n",
    "convert_sentence_train method takes a sentence(string) and unigram_counts(dictionary) as input,\n",
    "convert the sentence to the format which can be used to calculate unigram,bigram and trigram\n",
    "'''\n",
    "def convert_sentence_train(sentence,unigram_counts):\n",
    "    return ['<s0>',\"<s>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s>\",'</s0>']\n",
    "\n",
    "\n",
    "'''\n",
    "get_counts method takes sentences( a list of lists, where each list represents a sentence) as input,\n",
    "calculate unigram counts, bigram counts, trigram counts, token counts, store the results inside 3\n",
    "dictionaries and a float and return them.\n",
    "'''\n",
    "def get_counts(sentences):\n",
    "    unigram_counts = {}\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    trigram_counts=defaultdict(lambda : defaultdict(dict))\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
    "        i=0\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]]=\\\n",
    "                trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0)+1\n",
    "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "\n",
    "        i+=1\n",
    "        bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "        unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    unigram_counts[\"</s0>\"] = unigram_counts[\"<s0>\"]\n",
    "    return unigram_counts, bigram_counts,trigram_counts, token_count\n",
    "\n",
    "'''\n",
    "split_file method takes filename(string) as input, split the file into a list of lists,\n",
    "where each small list represents a sentence.\n",
    "'''\n",
    "def split_file(filename):\n",
    "    sentences=[]\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            sentence=tokenize(line)\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "'''\n",
    "check_for_unk_test method checks if the word in the test data is unknown.\n",
    "'''\n",
    "\n",
    "def check_for_unk_test(word,unigram_counts):\n",
    "    if word in unigram_counts and unigram_counts[word] > 0:\n",
    "        return word\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "'''\n",
    "convert_sentence_test method converts test data to the format we need.\n",
    "'''\n",
    "def convert_sentence_test(sentence,unigram_counts):\n",
    "    return ['<s0>',\"<s>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s>\",'</s0>']\n",
    "\n",
    "\n",
    "'''\n",
    "get_unigram_prob_addk\n",
    "input\n",
    "sentence: a list of strings\n",
    "unigram_counts: a dictionary of unigram counts\n",
    "token_count: the total number of tokens\n",
    "k: the parameter of unigram perplexity calculation\n",
    "vocab_count: vocabulary count\n",
    "\n",
    "output:\n",
    "the probability of a single sentence\n",
    "\n",
    "This method calculates the probability of a single sentence using add k smoothing\n",
    "'''\n",
    "def get_unigram_prob_addk(sentence, unigram_counts,token_count, k,vocab_count):\n",
    "    prob_sum=0\n",
    "    sentence=convert_sentence_test(sentence,unigram_counts)\n",
    "    for word in sentence:\n",
    "        prob_sum+=math.log((unigram_counts[word]+k)/ \\\n",
    "                           (token_count+k*vocab_count))\n",
    "    return prob_sum\n",
    "\n",
    "'''\n",
    "get_trigram_prob_backoff\n",
    "input\n",
    "sentence: a list of strings\n",
    "unigram_counts: a dictionary of unigram counts\n",
    "bigram_counts: a dictionary of bigram counts ( a dictionary of dictionaries)\n",
    "trigram_counts: a dictionary of trigram counts( a dictionary of dictionaries of dictionaries)\n",
    "token_count: float\n",
    "tri_lamda: the parameter of backoff smoothing\n",
    "\n",
    "output\n",
    "the probability of a sentence using trigram model and backoff smoothing\n",
    "\n",
    "'''\n",
    "def get_trigram_prob_backoff(sentence,unigram_counts,bigram_counts,trigram_counts, token_count,tri_lamda):\n",
    "    prob_sum=0\n",
    "    sentence=convert_sentence_test(sentence,unigram_counts)\n",
    "    leng=len(sentence)\n",
    "    for i in range(0,leng-2):\n",
    "        if trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0)!=0:\n",
    "            prob_sum+=math.log(trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]]/ \\\n",
    "                               float(bigram_counts[sentence[i]][sentence[i+1]]))\n",
    "        elif bigram_counts[sentence[i+1]].get(sentence[i+2],0)!=0:\n",
    "            prob_sum+=math.log(bigram_counts[sentence[i+1]][sentence[i+2]]/ \\\n",
    "                               float(unigram_counts[sentence[i+1]]))\n",
    "\n",
    "        else:\n",
    "            prob_sum+=math.log(unigram_counts[sentence[i+2]]/token_count)\n",
    "\n",
    "    return prob_sum\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "calculate_perplexity_unigram method calculates the perplexity of documents using unigram and add k smoothing\n",
    "input\n",
    "sentences: a list of lists\n",
    "unigram_counts: a dictionary of unigram counts\n",
    "token_count: a floating point number representing the total number of tokens\n",
    "k: the parameter of add k smoothing\n",
    "\n",
    "output\n",
    "perplexity of documents under unigram model and add k smoothing\n",
    "'''\n",
    "\n",
    "def calculate_perplexity_unigram(sentences, unigram_counts, token_count, k):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    vocab_count=len(unigram_counts)\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence)+4  # have to consider the start and the end token\n",
    "        total_log_prob += get_unigram_prob_addk(sentence, unigram_counts,token_count, k,vocab_count)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "'''\n",
    "calculate_perplexity_trigram method calculates the perplexity of documents using trigram and backoff smoothing\n",
    "input\n",
    "sentences: a list of lists\n",
    "unigram_counts: a dictionary of unigram counts\n",
    "bigram_counts: a dictionary of bigram counts ( a dictionary of dictionaries)\n",
    "trigram_counts: a dictionary of trigram counts( a dictionary of dictionaries of dictionaries)\n",
    "token_count: a floating point number representing the total number of tokens\n",
    "tri_lamda: the parameter of backoff smoothing\n",
    "\n",
    "output\n",
    "perplexity of documents under trigram model and back off smoothing\n",
    "'''\n",
    "def calculate_perplexity_trigram(sentences, unigram_counts,bigram_counts,trigram_counts, token_count,tri_lamda):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += get_trigram_prob_backoff(sentence,unigram_counts,bigram_counts,trigram_counts, token_count,tri_lamda)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "\n",
    "train_sentences=split_file('bitext-large.en')\n",
    "\n",
    "unigram_counts, bigram_counts,trigram_counts, token_count=get_counts(train_sentences)\n",
    "\n",
    "print 'unigram_counts, bigram_counts,trigram_counts, token_count completed'\n",
    "\n",
    "\n",
    "test_sentences = split_file('newstest2013.en')\n",
    "print \"unigram perplexity,using add k, k= 1,2,3\"\n",
    "print calculate_perplexity_unigram(test_sentences,unigram_counts,token_count,1)\n",
    "print calculate_perplexity_unigram(test_sentences,unigram_counts,token_count,2)\n",
    "print calculate_perplexity_unigram(test_sentences,unigram_counts,token_count,3)\n",
    "print \"trigram perplexity\"\n",
    "print calculate_perplexity_trigram(test_sentences,unigram_counts,bigram_counts,trigram_counts,token_count,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Analysis</b>\n",
    "For unknown words, during the training phase(extract unigrams, bigrams, trigrams), whenever we come across a new word, we assume it is an unknown word, and substitude it with \"UNK\". During the testing phase, for words that do not appear in unigram_counts or the value of unigram_counts for the words is 0, we substitude the words with \"UNK\". \n",
    "\n",
    "Since unigram_counts, bigram_counts, and trigram_counts are calculated inside the same function, the sentences are all padded with \"< s 0 >\"\"< s >\" \"< / s >\"\"< / s 0 >\", which is suitable for trigram calculation. \n",
    "\n",
    "For the perplexity calculation of unigram language model, we used add k smoothing, and print the perplexity results of k=1,k=2,k=3. As k gets larger, the final perplexity gets larger, and the likelihood of test data gets smaller. This reason for this may be illustrated by an example. Assume there are 5 different words in total in the training dataset, each appearing only once, and 4 different words in total in the test dataset, also with one appearance each. The probability of each word during the training phase is 1/5. If we calculate perplexity for test dataset using add 2, then it would roughly be  1/(((1+2）/(5+5*2))^4) ( square root is ommited, as it does not affect the trend.) Adding 3 would be 1/(((1+3）/(5+5*3))^4), as k gets larger, this number would get larger. Since the size our test dataset is much smaller than the training set, it is reasonable to make the above assumption.\n",
    "\n",
    "The perplexity of the testing dataset using trigram is much smaller than that using unigram. This agrees with our assumption that the test document makes more sense under the trigram model. Because when calculating the perplexity using trigram, the probability of each word is calculated based on its previous words, for example, if the previous words are \"she goes\", then the next word is highly likely to be \"to\". In similar manner, the probability of each word increases, and the likelihood of whole text gets larger, resulting in the perplexity getting smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e6b19e57aa3da7c48b1ef6e2b85ca973927dff8217d970a2dd6ebb6a"
   },
   "source": [
    "### Translation model\n",
    "\n",
    "The next step is to estimate translation model probabilities. For this we will use a word-based alignment model, e.g., *ibm1* to learn word-based translation probabilities using expectation maxisation. Your task is to obtain good quality word alignments from this data, which will require careful use of the word alignment models. You will want to training in both translation directions, and combining the alignments.\n",
    "\n",
    "You should use the *bitext-small* files for this purpose, and be aware that it may take a minute or two to train ibm1 on this data. Inspect some of the word alignments and translation probabilities (e.g., using a few common words in German such as *haus*) to see if your approach is working, and give some examples of output alignments that you find that are good and bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "signature": "1ca7263b4ac25b0217719feef6d248e2effda55992f24ea69a0f15b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation table 1\n",
      "haus,house 0.414602078104\n",
      "unser, our 0.208992664598\n",
      "im,in 0.134873601334\n",
      "gut,good 0.308055968157\n",
      "alignment 1\n",
      "AlignedSent(['steigt', 'gold', 'auf', '10.000', 'dollar', '?'], ['$', '10,000', 'gold', '?'], Alignment([(0, 1), (1, 2), (2, 1), (3, 1), (4, 0), (5, 3)]))\n",
      "AlignedSent(['san', 'francisco', '\\u2013', 'es', 'war', 'noch', 'nie', 'leicht', ',', 'ein', 'rationales', 'gespr\\xe4ch', '\\xfcber', 'den', 'wert', 'von', 'gold', 'zu', 'f\\xfchren', '.'], ['san', 'francisco', '\\u2013', 'it', 'has', 'never', 'been', 'easy', 'to', 'have', 'a', 'rational', 'conversation', 'about', 'the', 'value', 'of', 'gold', '.'], Alignment([(0, 0), (1, 1), (2, 2), (3, 3), (4, 6), (5, 1), (6, 5), (7, 7), (8, 8), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 8), (18, 11), (19, 18)]))\n",
      "AlignedSent(['und', 'es', 'kam', ',', 'wie', 'es', 'kommen', 'musste', '.'], ['wouldn\\u2019t', 'you', 'know', 'it', '?'], Alignment([(0, None), (1, 3), (2, 0), (3, 1), (4, 0), (5, 3), (6, 0), (7, 0), (8, None)]))\n",
      "AlignedSent(['allerdings', 'gab', 'es', 'im', 'januar', '1980', 'offensichtlich', 'einen', '\\u201efreak', 'peak\\u201c', 'in', 'einer', 'zeit', 'erh\\xf6hter', 'geopolitischer', 'instabilit\\xe4t', '.'], ['but', 'january', '1980', 'was', 'arguably', 'a', '\\u201cfreak', 'peak\\u201d', 'during', 'a', 'period', 'of', 'heightened', 'geo-political', 'instability', '.'], Alignment([(0, 7), (1, 7), (2, 7), (3, 1), (4, 1), (5, 2), (6, 7), (7, 7), (8, 7), (9, 7), (10, 4), (11, 4), (12, 10), (13, 12), (14, 13), (15, 14), (16, 15)]))\n",
      "AlignedSent(['eine', 'm\\xf6glichkeit', 'w\\xe4re', 'nat\\xfcrlich', 'der', 'totale', 'zusammenbruch', 'des', 'us-dollars', '.'], ['one', 'answer', ',', 'of', 'course', ',', 'is', 'a', 'complete', 'collapse', 'of', 'the', 'us', 'dollar', '.'], Alignment([(0, 7), (1, 8), (2, 9), (3, 4), (4, 10), (5, 8), (6, 9), (7, 10), (8, 8), (9, 14)]))\n",
      "translation table 2\n",
      "haus,house 0.575915154686\n",
      "unser, our 0.826289225148\n",
      "im,in 0.496432921368\n",
      "gut,good 0.336095656246\n",
      "alignment 2\n",
      "AlignedSent(['$', '10,000', 'gold', '?'], ['steigt', 'gold', 'auf', '10.000', 'dollar', '?'], Alignment([(0, 4), (1, 3), (2, 1), (3, 5)]))\n",
      "AlignedSent(['san', 'francisco', '\\u2013', 'it', 'has', 'never', 'been', 'easy', 'to', 'have', 'a', 'rational', 'conversation', 'about', 'the', 'value', 'of', 'gold', '.'], ['san', 'francisco', '\\u2013', 'es', 'war', 'noch', 'nie', 'leicht', ',', 'ein', 'rationales', 'gespr\\xe4ch', '\\xfcber', 'den', 'wert', 'von', 'gold', 'zu', 'f\\xfchren', '.'], Alignment([(0, 0), (1, 1), (2, 2), (3, 3), (4, 10), (5, 6), (6, 10), (7, 7), (8, 17), (9, 10), (10, 9), (11, 10), (12, 11), (13, 12), (14, 13), (15, 14), (16, 15), (17, 16), (18, 19)]))\n",
      "AlignedSent(['wouldn\\u2019t', 'you', 'know', 'it', '?'], ['und', 'es', 'kam', ',', 'wie', 'es', 'kommen', 'musste', '.'], Alignment([(0, 7), (1, 7), (2, 7), (3, 5), (4, 6)]))\n",
      "AlignedSent(['but', 'january', '1980', 'was', 'arguably', 'a', '\\u201cfreak', 'peak\\u201d', 'during', 'a', 'period', 'of', 'heightened', 'geo-political', 'instability', '.'], ['allerdings', 'gab', 'es', 'im', 'januar', '1980', 'offensichtlich', 'einen', '\\u201efreak', 'peak\\u201c', 'in', 'einer', 'zeit', 'erh\\xf6hter', 'geopolitischer', 'instabilit\\xe4t', '.'], Alignment([(0, 0), (1, 4), (2, 5), (3, 1), (4, 9), (5, 7), (6, 9), (7, 9), (8, 13), (9, 7), (10, 9), (11, 11), (12, 13), (13, 14), (14, 15), (15, 16)]))\n",
      "AlignedSent(['one', 'answer', ',', 'of', 'course', ',', 'is', 'a', 'complete', 'collapse', 'of', 'the', 'us', 'dollar', '.'], ['eine', 'm\\xf6glichkeit', 'w\\xe4re', 'nat\\xfcrlich', 'der', 'totale', 'zusammenbruch', 'des', 'us-dollars', '.'], Alignment([(0, 5), (1, 5), (2, 9), (3, 7), (4, 3), (5, 9), (6, 1), (7, 0), (8, 5), (9, 6), (10, 7), (11, 4), (12, 8), (13, 8), (14, 9)]))\n",
      "combined translation table\n",
      "haus,house 0.99051723279\n",
      "unser, our 1.03528188975\n",
      "im,in 0.631306522702\n",
      "gut,good 0.644151624403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "read_bitext methods reads two files and returns a list of aligned sentences\n",
    "'''\n",
    "def read_bitext(file_name1,file_name2):\n",
    "    bitext=[]\n",
    "    with open(file_name1,'r') as f1:\n",
    "        with open(file_name2,'r') as f2:\n",
    "            for line1,line2 in zip(f1,f2):\n",
    "                list1=tokenize(line1)\n",
    "                list2=tokenize(line2)\n",
    "                bitext.append(AlignedSent(list1,list2))\n",
    "    return bitext\n",
    "\n",
    "'''\n",
    "Transform the translation table of English to German, for easier processing in the next steps\n",
    "'''\n",
    "\n",
    "def n_translation_table(t_en_de):\n",
    "    n_t_en_de=defaultdict(dict)\n",
    "    for en in t_en_de:\n",
    "        for de in t_en_de[en]:\n",
    "            if de in n_t_en_de:\n",
    "                n_t_en_de[de][en]=n_t_en_de[de].get(en,0)+t_en_de[en][de]\n",
    "            else:\n",
    "                n_t_en_de[de][en]=t_en_de[en][de]\n",
    "    return n_t_en_de\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "combine_translation_table takes two translation tables(from en to de and from de to en),\n",
    "unigram_counts and token_count as input, combine the two tables together\n",
    "and return the combined table. The table are combined by adding p(e|f) and p(f|e)p(e)\n",
    "together\n",
    "'''\n",
    "def combine_translation_table(t1,t2):\n",
    "    t=defaultdict(dict)\n",
    "    \n",
    "    for word1 in t1:\n",
    "        if word1 in t2:\n",
    "            for word2 in t1[word1]:\n",
    "                t[word1][word2]=t1[word1][word2]+\\\n",
    "                t2[word1].get(word2,0)\n",
    "        else:\n",
    "            for word2 in t1[word1]:\n",
    "                t[word1][word2]=t1[word1][word2]\n",
    "    return t\n",
    "\n",
    "bitext1=read_bitext('bitext-small.de','bitext-small.en')\n",
    "\n",
    "ibm1 = IBMModel1(bitext1, 5)\n",
    "t1=ibm1.translation_table\n",
    "\n",
    "\n",
    "print 'translation table 1'\n",
    "print 'haus,house',t1['haus']['house']\n",
    "print 'unser, our',t1['unser']['our']\n",
    "print 'im,in',t1['im']['in']\n",
    "print 'gut,good',t1['gut']['good']\n",
    "\n",
    "print 'alignment 1'\n",
    "for i in range(0,5):\n",
    "    print `bitext1[i]`\n",
    "\n",
    "\n",
    "bitext2=read_bitext('bitext-small.en','bitext-small.de')\n",
    "\n",
    "ibm1reversed = IBMModel1(bitext2, 5)\n",
    "t2=ibm1reversed.translation_table\n",
    "\n",
    "\n",
    "print 'translation table 2'\n",
    "print 'haus,house',t2['house']['haus']\n",
    "print 'unser, our',t2['our']['unser']\n",
    "print 'im,in',t2['in']['im']\n",
    "print 'gut,good',t2['good']['gut']\n",
    "\n",
    "print 'alignment 2'\n",
    "for i in range(0,5):\n",
    "    print `bitext2[i]`\n",
    "\n",
    "n_t2=n_translation_table(t2)\n",
    "t_combined=combine_translation_table(t1,n_t2)\n",
    "\n",
    "print 'combined translation table'\n",
    "print 'haus,house',t_combined['haus']['house']\n",
    "print 'unser, our',t_combined['unser']['our']\n",
    "print 'im,in',t_combined['im']['in']\n",
    "print 'gut,good',t_combined['gut']['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Analysis </b>\n",
    "\n",
    "In the above code, we firstly read the bitext in and store it in the format required by the IBM models. Then IBM model1 is trained and a translation table is ourputed. To get a shallow understanding of the quality of the translation table, we printed the translation probablity for some common words, namely \"haus\" and \"house\", \"unser\" and \"our\", \"im\" and \"in\", \"gut\" and \"good\". Form the output, we can see the probability of these word pairs are relatively high, meaning that the model realizes they are corresponding pairs in the two languages. \n",
    "\n",
    "For the sentence alignments, the first 5 alignments are printed. As we can see from the results, the overall alignment is not bad. For the first sentence, it successfully aligned the dollar sign with dollar, gold with gold. For the second sentence, it successfully aligned it with es, rationales with rational, wert with value and so on. When the model is trained backwards, the results are similar. \n",
    "\n",
    "To combine the two translation tables together, we are simply summing p(e|f) and p(f|e) together, where p(e|f) according our code is translation table t1, p(f|e) is translation table t2. To illustrate this, I'll use an example.  t1 represents the probability of haus translated to house, t2 represents the probability of house translating to haus, since the models for these two tables are trained in different order, there may be some difference between them, by summing them together, we manage to minimize the errors of a single model. Although the results of this would be in the range of 0~2, since we only care about the relative probabilities compared to other translations, this would be fine. \n",
    "\n",
    "From the combined probabilities, we can see that although the probability of 'unser' being translated into 'our' is low in translation table 1, now after combining the probability from table 2, the probability become higher. And the same rule applies to all the other words. This combination minimize the errors of a single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "44505ffd562b8a32a76bab591e1d6faf12e48c4331bc56771f1b6af4"
   },
   "source": [
    "### Decoding \n",
    "\n",
    "Finally you'll need to solve for the best translation for a given string. For this you should do simple word-for-word translation where each word is translated independently. Use the alignments learned above (or the translation parameters) to translate each word of the queries into English. Compare taking the maximum probability of translation into English $p(e|f)$, with the noisy-channel probability, $p(f|e)p(e)$, which considers the reverse translation and a unigram language model probability. (You'll get a chance later to do something more ambitious, using the trigram model.)\n",
    "\n",
    "Use your algorithms to translate the first 100 queries into English, and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "signature": "fd832cc8c7b41286a9c19ef620c5c9add5d26c7f56c589c8b97a005b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translated queries 1, using maximum p(e|f)\n",
      "['82', 'deterring', '(', 'von', 'guises', '.', 'affirmative', ':', 'subset', 'which', 'abhor', 'which', 'movement', ')', 'is', 'arrive', '', 'gambler\\\\u2019s', '', 'which', 'into', '\\\\u201cabenomics\\\\u201d', 'deterring', 'neuro-scientist', 'deterring', 'external', 'abhor', 'von', '5-7', '', 'remnants', 'struggle', '-', 'and', '', 'embryonically', 'and', 'states-china', 'becomes', '.']\n",
      "['116', 'chronicles', '(', '', ':', '', 'for', '', 'bulletin', 'cambridge', '', 'which', 'outdated', '', 'for', 'bulletin', 'cambridge', '', ')', 'is', 'embarked', 'befuddled', 'deterring', 'en', '.']\n",
      "['240', 'deterring', 'von', '', '', 'which', '``', 'really', '``', 'which', 'also', '', 'which', '', '-', 'or', '', 'which', 'english', '', 'which', 'is', 'chronicles', 'fundamental', 'scientific', 'method', 'into', 'deterring', '.']\n",
      "['320', 'chronicles', '(', 'faros', 'el', 'which', 'von', '', '', 'which', '-', '``', 'together', '-', '``', 'which', '``', '', '``', 'which', 'immigration\\\\u2019s', 'is', '``', 'chronicles', '', '', '``', ')', 'which', 'is', 'arrive', '', 'which', 'nba', 'syndrome', 'blood', 'contingent', 'heart', '', '.']\n",
      "['540', 'under', 'deterring', 'statutes', 'summarizes', 'you', 'chronicles', 'three', 'contrary', 'northern', '', 'roes', '', '', 'which', '', 'and', '', 'together', '.']\n",
      "unigram perplexity\n",
      "1678.05342574\n",
      "trigram perplexity\n",
      "1218.30015988\n",
      "translated queries 2,using maximum p(f|e)p(e)\n",
      "['82', 'the', '(', 'of', 'as', '.', 'action', ',', ',', ',', 'and', ',', 'the', ')', 'is', 'a', '', 'the', '', ',', 'in', 'what', 'the', ',', 'the', 'the', 'and', 'of', 'their', '', \"'s\", 'the', ',', 'and', '', 'and', 'and', 'in', 'will', '.']\n",
      "['116', 'the', '(', '', ',', '', 'for', '', 'at', '\\\\u2013', '', ',', 'when', '', 'for', 'at', '\\\\u2013', '', ')', 'is', 'a', 'and', 'the', 'they', '.']\n",
      "['240', 'the', 'of', '', '', ',', ',', 'it', ',', ',', ',', '', ',', '', ',', 'or', '', ',', ',', '', ',', 'is', 'the', ',', 'a', 'the', 'in', 'the', '.']\n",
      "['320', 'the', '(', 'will', ',', ',', 'of', '', '', ',', ',', ',', ',', ',', ',', ',', ',', '', ',', ',', 'on', 'is', ',', 'the', '', '', ',', ')', ',', 'is', 'a', '', ',', 'which', 'the', 'of', 'the', 'of', '', '.']\n",
      "['540', ',', 'the', 'the', 'the', '.', 'the', ',', 'in', 'are', '', 'the', '', '', ',', '', 'and', '', ',', '.']\n",
      "unigram perplexity\n",
      "52.6580625595\n",
      "trigram perplexity\n",
      "105.832933827\n",
      "translated queries 3,using combine translation table\n",
      "['82', 'the', '(', 'from', 'guises', '.', 'action', ':', 'indeed', ',', 'rush', ',', 'movement', ')', 'is', 'a', '', 'of', '', ',', 'in', 'what', 'the', 'neuro-scientist', 'the', 'external', 'rush', 'from', '5-7', '', 'remnants', 'fight', '-', 'and', '', 'pushed', 'and', 'states-china', 'will', '.']\n",
      "['116', 'the', '(', '', ':', '', 'for', '', 'bulletin', '\\\\u2013', '', ',', 'outdated', '', 'for', 'bulletin', '\\\\u2013', '', ')', 'is', 'a', 'befuddled', 'the', 'mass', '.']\n",
      "['240', 'the', 'from', '', '', ',', '``', 'really', '``', ',', 'also', '', ',', '', '-', 'or', '', ',', 'english', '', ',', 'is', 'the', 'fundamental', 'scientific', 'method', 'in', 'the', '.']\n",
      "['320', 'the', '(', 'greek', 'el', ',', 'from', '', '', ',', '-', '``', 'together', '-', '``', ',', '``', '', '``', ',', 'meant', 'is', '``', 'the', '', '', '``', ')', ',', 'is', 'a', '', ',', 'which', 'syndrome', 'blood', 'from', 'heart', '', '.']\n",
      "['540', 'under', 'the', 'as', 'summarizes', 'you', 'the', 'three', 'in', 'northern', '', 'underlying', '', '', ',', '', 'and', '', 'together', '.']\n",
      "unigram perplexity\n",
      "367.198538754\n",
      "trigram perplexity\n",
      "230.514334427\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "decoder_de_to_en method takes t_de_en( a translation table from de to en) and q_list(a list of sentences to be\n",
    "translated) as input, translats the sentences into en by choosing the word with maximum p(e|f). If the word in\n",
    "the query does not exist in the translation table, it is translated into an empty string.\n",
    "'''\n",
    "\n",
    "def decoder_de_to_en(t_de_en, q_list):\n",
    "    translated=[]\n",
    "    for sent in q_list:\n",
    "        #print sent\n",
    "        tran_sent=[sent[0]]\n",
    "        for de_word in sent[1:]:\n",
    "            best_tran=''\n",
    "            if de_word in t_de_en:\n",
    "                best_prob=0\n",
    "                for en_word in t_de_en[de_word]:\n",
    "                    if t_de_en[de_word][en_word]>best_prob:\n",
    "                        best_prob=t_de_en[de_word][en_word]\n",
    "                        best_tran=en_word\n",
    "            tran_sent.append(best_tran)\n",
    "        #print tran_sent\n",
    "        translated.append(tran_sent)\n",
    "    return translated\n",
    "\n",
    "'''\n",
    "decoder_en_en_to_de method takes t_en_de( a translation table from en to de), q_list( a list of sentences to\n",
    "be translated), and unigram_counts as input, translates the q_list into en by choosing the word with maximum\n",
    "P(f|e)p(e). If the word in the query does not exist in the translation table, it is translated into an empty\n",
    "string.\n",
    "'''\n",
    "\n",
    "def decoder_en_en_to_de(n_t_en_de,q_list,unigram_counts):\n",
    "    \n",
    "    translated=[]\n",
    "    for sent in q_list:\n",
    "        tran_sent=[sent[0]]\n",
    "        for de_word in sent[1:]:\n",
    "            best_tran=''\n",
    "            if de_word in n_t_en_de:\n",
    "                best_prob=0\n",
    "                for en_word in n_t_en_de:\n",
    "                    # we are dividing unigram_counts by token counts because we only care about\n",
    "                    #the relative amount\n",
    "                    prob= unigram_counts.get(en_word,0)*n_t_en_de[de_word].get(en_word,0)\n",
    "                    if prob>=best_prob:\n",
    "                        best_prob=prob\n",
    "                        best_tran=en_word\n",
    "            tran_sent.append(best_tran)\n",
    "        translated.append(tran_sent)\n",
    "\n",
    "    return translated\n",
    "\n",
    "quries=split_file('dev.queries')\n",
    "\n",
    "# translated queries 1 is calculated by choosing the largest p(e|f)\n",
    "trans_quries1=decoder_de_to_en(t1,quries[0:100])\n",
    "# translated queries 2 is calculated by choosing the largest p(f|e)p(e)\n",
    "trans_quries2=decoder_en_en_to_de(n_t2,quries[0:100],unigram_counts)\n",
    "# translated queries 3 is calculated by choosing the largest p(e|f) using the combined table\n",
    "trans_quries3=decoder_de_to_en(t_combined,quries[0:100])\n",
    "\n",
    "print 'translated queries 1, using maximum p(e|f)'\n",
    "for i in range(0,5):\n",
    "    print trans_quries1[i]\n",
    "print 'unigram perplexity'\n",
    "print calculate_perplexity_unigram(trans_quries1,unigram_counts,token_count,1)\n",
    "print 'trigram perplexity'\n",
    "print calculate_perplexity_trigram(trans_quries1,unigram_counts,bigram_counts,trigram_counts,token_count,1)\n",
    "    \n",
    "print 'translated queries 2,using maximum p(f|e)p(e)'\n",
    "for i in range(0,5):\n",
    "    print trans_quries2[i]\n",
    "print 'unigram perplexity'\n",
    "print calculate_perplexity_unigram(trans_quries2,unigram_counts,token_count,1)\n",
    "print 'trigram perplexity'\n",
    "print calculate_perplexity_trigram(trans_quries2,unigram_counts,bigram_counts,trigram_counts,token_count,1)\n",
    "\n",
    "    \n",
    "print 'translated queries 3,using combine translation table'\n",
    "for i in range(0,5):\n",
    "    print trans_quries3[i]\n",
    "print 'unigram perplexity'\n",
    "print calculate_perplexity_unigram(trans_quries3,unigram_counts,token_count,1)\n",
    "print 'trigram perplexity'\n",
    "print calculate_perplexity_trigram(trans_quries3,unigram_counts,bigram_counts,trigram_counts,token_count,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "d9ee5ceedefb6b88a2d202b764bb850957691bb43dcdee277f0721d8"
   },
   "source": [
    "Now you will need to write some text about the procedure above, and how well it worked. You should answer:\n",
    "  - how good are the translations? How do your methods fare at translating the content words versus functions words? You can use Google translate to provide an alternative (pretty good) translation for comparison purposes. \n",
    "  - what are good settings for various modelling parameters such as language model discounting, translation model smoothing, any decoding parameters, and how do these affect the outputs?\n",
    "  - compare the language model perplexity for the unigram and trigram models over your decoder outputs, how does the difference in perplexity between the two models compare to your test above on the monolingual data? \n",
    "  - how do the learned alignments differ between the two translation directions, and does their combination appear oto improve the predictions? (You don't need a formal evaluation here, as we have no *gold standard* alignments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "a7c8de897d3560c283122462378569717d937d62d1804cbbede6634f"
   },
   "source": [
    "<b> Analysis </b>\n",
    "\n",
    "In the above code, we used 3 methods to translate the queries. The first method simply chooses the translation with largest p(e|f), using the translation table of German to English. The second method chooses the translation with largest p(f|e)p(e), using translation table of English to German, and unigram_counts dictionary. The third method uses the combined translation table(the table is combined by adding p(e|f) and p(f|e) together), and chooses the translation with largest probability. \n",
    "A brief glipse of the results shows that the second method is of lowest quality, and is more suitable for translating function words than content words, since the output we get actually consists of only function words, which would be of no use for querying corpus. Actually all the methods are better at translating the function words, this is because function words are much more common during training, and our model has good knowledge of these words.\n",
    "\n",
    "To evaluate the quality of translation method 1 and 3, the results of google translation of the same queries are shown here:\n",
    "82 (. Of eng action: act, action, movement,) is a film genre of entertainment cinema, in which the continued transition of the external action of mostly spectacularly staged battle - and violent scenes is advanced and illustrated.\n",
    "\n",
    "116 (unit sign: u for unified atomic mass unit, amu outdated for atomic mass unit) is a unit of measure of mass.\n",
    "\n",
    "240 of actualis from Latin, \"real\", even actuality principle, uniformity - or gleichförmigkeitsprinzip, English uniformitarianism, is the basic scientific method in.\n",
    "\n",
    "320 (Greek el, from Ancient Greek grc, - \"together -\", \"tie\", is meant \"the heart bag attached\") is a blood vessel that leads away the blood from the heart.\n",
    "\n",
    "540 under the designation one summarizes the three lying in the northern waters alpenvorland units obersee, subsea and Seerhein together.\n",
    "\n",
    "After carefully comparison, we found out the performance method 3 is slightly better than that of method 1. For example, the translation using method 3 translates \"action\", \"mass\", \"from\" and so on correctly. Nonetheless, the performance of both models are actually poor. This is because we are translating only by words, and the training dataset is too small that many words we meet during translation are not contained in translation tables.\n",
    "\n",
    "The perplexity of method 2 is the smallest, since a lot of words did not get translated in method 2 and is replaced with an empty string(which will converted to \"UNK\" during calculation), and the only words get translated are all function words, of which the word probabilities are high, this results in an abnormal perplexity score. \n",
    "\n",
    "The unigram perplexity scores of method 1 and 3 are actually lower than that of monolingual data, this is because during translation, the word with higher probability of appearance is chosen, and many words did not get translated. Unigram perplexity does not take any word order into consideration, therefore, a bag of high frequency words has smaller unigram perplexity. \n",
    "\n",
    "The trigram perplexity scores of method 1 is higher than method 3, and the score of method 3 is similar to that of monolingual data. The good score of method 3 is probably because many word is translated into empty strings, and these empty strings are turned into \"UNK\" during calculation, and the probability of \"UNK\" relatively high. In addition, many words do not appear in the trigram models, thus our backoff algorithm will use bigram, then unigram, which will discard the order information gradually. \n",
    "\n",
    "The performance of a combination of two translation directions is slightly better by examining the first 5 translated queries, and also by comparing the perplexity score, the reason for this has already been illustrated above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "acb9112859031d87f7183674120ff9f4fcbb4306cc6a4ca3c75c2c81"
   },
   "source": [
    "## Putting the CLIR system together (10% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "0e59f726cc569cf42fc79cba180391cdb298fff20acc92d74f1e25a1"
   },
   "source": [
    "Now you should couple the translation and IR components from above. Take your translated queries and use these with the IR system to find the most relevant documents. You should then evaluate these predictions against the supplied relevance judgements, using the *mean average precision* (MAP) metric. As part of this, you will want to consider tuning the settings of the IR system for best performance, and comment on how successful your approach was and evaluate the IR performance is affected by the parameter settings and modelling decisions you have made.\n",
    "\n",
    "Note that if you were stuck on the translation steps above, or your translations are otherwise unusable, you can implement this step using google translate outputs for the 100 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "signature": "ef91974d67fdbecbc6aba960302ae94d368c8efb61792919cc5078ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1 1 b 0.5 k3 0 map 0.173657625218\n",
      "k1 1 b 0.5 k3 2 map 0.162724869763\n",
      "k1 1 b 0.9 k3 0 map 0.17159011618\n",
      "k1 1 b 0.9 k3 2 map 0.156905172064\n",
      "k1 1.5 b 0.5 k3 0 map 0.180242378134\n",
      "k1 1.5 b 0.5 k3 2 map 0.172917969815\n",
      "k1 1.5 b 0.9 k3 0 map 0.172733374121\n",
      "k1 1.5 b 0.9 k3 2 map 0.163368747105\n",
      "k1 2 b 0.5 k3 0 map 0.185343637229\n",
      "k1 2 b 0.5 k3 2 map 0.176912458099\n",
      "k1 2 b 0.9 k3 0 map 0.17533658329\n",
      "k1 2 b 0.9 k3 2 map 0.161841381465\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "read_query_rel method reads in query relevance file and processes it into a dictionary of dictionaries,\n",
    "where keys are the query id and document ids, and the values are the relevance levels\n",
    "'''\n",
    "def read_query_rel(filename):\n",
    "    query_rel=defaultdict(dict)\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            line=line.split('\\t')\n",
    "            query_rel[line[0]][line[2]]=line[3]\n",
    "\n",
    "    return query_rel\n",
    "\n",
    "\n",
    "'''\n",
    "map method calculates mean average precision of the first k query results using query_rel dictionary.\n",
    "'''\n",
    "def map(query_results,query_rel,k):\n",
    "\n",
    "    map1=0\n",
    "    for query in query_results:\n",
    "        rel_levels=0\n",
    "        rel_num=0\n",
    "        precision_sum=0\n",
    "        for i,docid in enumerate(query_results[query][0:k]):\n",
    "            if docid in query_rel[query]:\n",
    "\n",
    "                rel_num+=1\n",
    "                #print 'rel_num', rel_num\n",
    "                precision=rel_num/float(i+1)\n",
    "                #print 'precision',precision\n",
    "                precision_sum+=precision\n",
    "\n",
    "        if rel_num==0:\n",
    "            ave_precision=0\n",
    "        else:\n",
    "            ave_precision=precision_sum/rel_num\n",
    "            #print 'ave_precision',ave_precision\n",
    "\n",
    "\n",
    "        #if ave_precision>2:\n",
    "            #print query,query_results[query][0:k]\n",
    "        map1+=ave_precision\n",
    "\n",
    "\n",
    "\n",
    "    map1=map1/float(100)\n",
    "    return map1\n",
    "\n",
    "# We tuned some parameters to see if it changes the output map\n",
    "query_rel=read_query_rel('dev.qrel')\n",
    "for k1 in [1,1.5,2]:\n",
    "    for b in [0.5,0.9]:\n",
    "        for k3 in [0,2]:\n",
    "            total_result=dict()\n",
    "            for i,query in enumerate(trans_quries3):\n",
    "                total_result[query[0]]=bm25(tdf,df,doc_length,num_doc,doc_length_ave,query[1:],k1,b,k3)\n",
    "\n",
    "            \n",
    "            map1=map(total_result,query_rel,100)\n",
    "\n",
    "            print'k1',k1,'b',b,'k3',k3,'map', map1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eaf51c896965fa37936a7cf850206a514011e5199be1758b6bad2882"
   },
   "source": [
    "<b> Analysis </b>\n",
    "\n",
    "As we can see, the mean average precision is always low. We tried 3 values for k1, namely 1,1.5, 2, two values for b, 0.5, 0.9, two values for k3, 0 and 2. The mean average precision is calculated using the first 100 results for each query. As can be seen from the outputs, the results does not change much in terms of mean average precision. This is probably because the translation results for the queries are already poor and almost not related to the original queries, therefore, no matter how the parameters are tuned, it cannot improve the search results.\n",
    "\n",
    "However, an interesting fact to notice is that the result of k3=0 is always better than that of k3=2. k3 controls the influence of the frequency of terms in the query. k3=0 means the frequency of terms in the query does not influence the final results at all, and as k3 gets larger, the more frequent words play a more important role in the querying. From our result, we can see that k3 is better set as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "8ffdae9206077d7f4b274e45a78eaf5115bc5720e93cfb96a203cf56"
   },
   "source": [
    "Note that in a complete CLIR you would translate the retreived documents back into German. We won't bother about that for this assignment, especially as I doubt many of you can understand German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a87e15afb78c449609a14ad168103934d570f356f0f60ee10413c0d0"
   },
   "source": [
    "# Part 2: Extension and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c7a87c48728bf2c30af7a51f1d196824e100b4d2c64960c75ae3d119"
   },
   "source": [
    "## Extension (25% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a8579c28c4566e444135477d71daf3d2adcc41dc436752634cba08b6"
   },
   "source": [
    "You now have a working end-to-end CLIR system. The next step is to revisit some of the steps above to see if you can improve the performance of the system. You are invited to be creative with your ideas about how to do this. Note that many great ideas might not produce improvements, or end up being very difficult to implement. This will not be assessed solely on the grounds of CLIR accuracy; we will be looking primarily for interesting and creative ideas.\n",
    "\n",
    "Here are some ideas on what you might do. Warning: some are bigger than others, aim to spend 5-6 hours in total on this.\n",
    "\n",
    "#### Structured decoding\n",
    "\n",
    "Decode using a higher order language model, possibly with word reordering. Using your trigram language model and a word-based or phrase-based translation model, search for the best scoring translation sentence. You may want to use the tools in *nltk.translate*, including the phrase extraction and *StackDecoder*, especially if you wish to support word reordering. The decoding algorithm is much simpler (and tractable) without reordering, so you might want to implement this yourself. You will want to work with log-probabilities to avoid numerical underflow. Note  that the *StackDecoder* is not highly stable code, so you will need to understand its code in detail if you chose to use it. You will need to supply translation log probabilities and language model log probabilities as input to the decoder and consider the context of the previous two words in estimating the probability of each word. \n",
    "\n",
    "#### Probabilisitc querying\n",
    "\n",
    "Consider queries including weighted terms based on multiple translation outputs, such as the word translation probabilities or using *k-best* translations from a decoder (you will need to extract not just the best translation from the decode, but the runners up, which can be done from the stack contents after beam search.)  You should consider how best to define TF and DF for query words based on their ambiguous translations. You may find inspiration in the bibliography of [Douglas Oard](https://terpconnect.umd.edu/~oard/research.html#mlia) such as [this paper](https://terpconnect.umd.edu/~oard/pdf/coling12ture.pdf).\n",
    "\n",
    "#### Word alignment\n",
    "\n",
    "Implement an extended word alignment model, such as the *hmm* model (see [Vogel's paper](http://www.aclweb.org/anthology/C96-2141)) or a variant of *ibm2*. The variant of *ibm2* could include a better formulation for the distortion term, $p(i|j,I,J)$, based around rewarding values of i and j that are at similar relative positions to encourage alignments near the diagonal. This contrasts with *ibm2* which just learns a massive table of counts for all combinations of i,j,I and J. This strictly needs normalisation, as its not a probability, but for simplicity you can ignore this aspect here. This idea is inspired by [fast-align](https://github.com/clab/fast_align) which has a similar, but slightly more complex, formulation.\n",
    "\n",
    "You might also want to experiment with [fast-align](https://github.com/clab/fast_align) (an extremely fast and accurate variant of *ibm2*) or [GIZA++](http://www.statmt.org/moses/giza/GIZA++.html) (an implementation of *ibm1* - *ibm5* and the *hmm*), both widely used and highly optimised alignment tools. Neither of these tools are in python, and may be a little involved to install and compile.\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "Experiment with a state-of-the-art translation tool like [Moses](http://www.statmt.org/moses/).  Can you get better CLIR performance using this, trained on the full parallel dataset? You might want to consider translating the document collection into German, and then doing the IR entirely in German. You might consider building a python interface to moses, building on the existing very basic funcationality. **Warning:** Moses is a complex suite of software, and can be a little complex to install and use. However there are good installation and usage guides on the moses webpage.\n",
    "\n",
    "#### IR engine\n",
    "\n",
    "There are several great open source IR engines, including [Lucene](https://lucene.apache.org/core/), [Terrier](http://terrier.org/), [Zettair](http://www.seg.rmit.edu.au/zettair/) and [Lemur](http://www.lemurproject.org/). You could experiment with these, and use this to compare the accuracy of different types of IR models on the CLIR data. Alternatively, you could implement a faster or more compact retreival system in python, e.g., using compressed vbyte encodings, skip lists or similar.\n",
    "\n",
    "Note that the development of some of the extensions maye require working in the command shell and in other languages than python. If this is the case, you should attach your code in separate files as part of your submission, but include the analysis text here. *Note that excellent submissions implementing interfaces or models not currently in NLTK will be considered for inclusion into the toolkit.* You should include both the code, and text explaining your work and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "signature": "4537a8574377d3dc679a2045491857ab196fce1ab6512c9bac7c37e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start IBMModel1\n",
      "['our', 'in']\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [')', 'is', 'a'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [')', 'is'], [], []]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['the'], [], [], [], []]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [')', ','], [], [], [], [], [], [], [], []]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print \"start IBMModel1\"\n",
    "bitext2=read_bitext('bitext-small.en','bitext-small.de')\n",
    "#train the model so that we get initial alignments\n",
    "model_ext = IBMModel1(bitext2, 5)\n",
    "\n",
    "phrase_pair_counts = defaultdict(Counter)\n",
    "for s in bitext2:\n",
    "    #print `s`\n",
    "    flag=False\n",
    "    # some of the alignments contains none, which will make the phrase extraction throw exceptions\n",
    "    # we simply discard them\n",
    "    for i,align in enumerate(s.alignment):\n",
    "        for num in align:\n",
    "            if num==None:\n",
    "                flag=True\n",
    "    #print `s`\n",
    "    if not flag:\n",
    "        phrases = phrase_extraction(' '.join(s.words), ' '.join(s.mots), s.alignment, max_phrase_length=3)\n",
    "        #print phrases\n",
    "        for en_span, fr_span, en_phrase, fr_phrase in phrases:\n",
    "                for en_span, fr_span, en_phrase, fr_phrase in phrases:\n",
    "                    phrase_pair_counts[tuple(en_phrase.split())][tuple(fr_phrase.split())] += 1\n",
    "#Calculate a phrase table\n",
    "phrase_table = PhraseTable()\n",
    "for en_phrase, fr_counts in phrase_pair_counts.items():\n",
    "    total = sum(fr_counts.values())\n",
    "    for fr_phrase, count in fr_counts.items():\n",
    "        phrase_table.add(fr_phrase, en_phrase, math.log(count / float(total)))\n",
    "\n",
    "\n",
    "# calculate a language probability model\n",
    "language_prob = defaultdict(lambda: -999.0)\n",
    "total_words = sum(unigram_counts.values())\n",
    "for word_type, count in unigram_counts.items():\n",
    "    if count>0:\n",
    "        language_prob[word_type] = math.log(count / float(total_words))\n",
    "\n",
    "# I tried to implement a decoder myself, which will take phrase table, and trigram_counts as \n",
    "#input, and translate the sentence based on phrase translation, but it turns out that there are \n",
    "#so many conditions to consider and the process is too complicated, the following is an implementation\n",
    "#that is not yet finished\n",
    "def my_decoder(phrase_table, trigram_counts, bigram_counts, unigram_counts, query):\n",
    "    i=0\n",
    "    best_tran_query=[]\n",
    "    while i < (len(query)-2):\n",
    "        best_tran=[]\n",
    "        best_prob=-9e99\n",
    "        try:\n",
    "            trans=phrase_table.translations_for(tuple(query[i:i + 3]))\n",
    "\n",
    "            for a_trans in trans:\n",
    "                prob_trans=a_trans[1]\n",
    "                trans_words=list(a_trans[0])\n",
    "                if len(trans_words)==3:\n",
    "                    if trigram_counts[trans_words[0]][trans_words[1]].get(trans_words[2], 0)!=0:\n",
    "                        prob_lan=math.log(trigram_counts[trans_words[0]][trans_words[1]][trans_words[2]] / \\\n",
    "                                          float(bigram_counts[trans_words[0]][trans_words[1]]))\n",
    "\n",
    "                else:\n",
    "                    prob_lan=-9e99\n",
    "\n",
    "                prob=prob_lan+prob_trans\n",
    "                if prob>=best_prob:\n",
    "                    best_tran=trans_words\n",
    "\n",
    "\n",
    "            i+=3\n",
    "\n",
    "        except KeyError:\n",
    "            i+=1\n",
    "\n",
    "\n",
    "        best_tran_query.append(best_tran)\n",
    "    return best_tran_query\n",
    "\n",
    "\n",
    "# wrapper class\n",
    "class LM:\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs\n",
    "    def probability_change(self, context, phrase):\n",
    "        # Used when expanding a hypothesis with a new phrase\n",
    "        # (higher order LMs would need to look at the word sequence, including context)\n",
    "        return sum([self.probs[word] for word in phrase])\n",
    "    def probability(self, phrase):\n",
    "        # Used for future cost estimation only, to get a cheap (approximate) LM score for each phrase\n",
    "        return sum([self.probs[word] for word in phrase])        \n",
    "        \n",
    "    \n",
    "\n",
    "language_model = LM(language_prob)\n",
    "# this stack_decoder fails when the words in the queries do not appear in the phrase table or\n",
    "#language model\n",
    "stack_decoder = StackDecoder(phrase_table, language_model)\n",
    "# it only works with some common words\n",
    "print stack_decoder.translate(['unser','im'])\n",
    "# our not yet finished decoder\n",
    "for query in quries[0:5]:\n",
    "\n",
    "    print my_decoder(phrase_table, trigram_counts, bigram_counts, unigram_counts, query[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "848ed26611b0b691bf0b1e19b9a5ff6bb51a5a4750ee3b6dd9c88b8f"
   },
   "source": [
    "<b> Analysis </b>\n",
    "\n",
    "For this part, I tried the first option \"Structured decoding\". The stack decoder from nltk does not work when the words from queries do not exist in the phrase table or language model. Therefore I chose to implement a decoder myself. The initial alignments is calculated using IBM model 1, then these alignments used to calculate a phrase table. The two inputs to the decoder should be phrase table and language model. Since we intend to translate by phrase, the language model should be a table of trigram language probabilities. However, our training data and computing resource is very limited, which means the trigram model can not cover all the phrases in the queries. Ideally, when the phrase does not exist in trigram, we should turn to bigram, then unigram. The probability of a word is calculated by multiplying its phrase table probability and trigram probability giving its previous two words. And the translation process is to choose the word which gives the largest probability. \n",
    "\n",
    "Unfortunately, the implementation of this decoding algorithm turns out to be too complicated. We can represent this by pseudo code(in ideal case, when the 3-word-long phrase exists in phrase table and trigram model)\n",
    "\n",
    "\n",
    "for a phrase(3 words long) in queries:\n",
    "\n",
    "    if phrase(3 words long) in phrase table:\n",
    "        for all the phrase translations in phrase table:\n",
    "            if phrase translation in trigram model:\n",
    "                probability of translation =phrase table probability*trigram probability\n",
    "\n",
    "\n",
    "then the phrase translation with largest probability product is chosen. \n",
    "\n",
    "However, it is possible that the phrase does not exist in phrase table, for this case, we have to first shorten the phrase to 2 words first, then check if it exist in phrase table\n",
    "     \n",
    "     else:\n",
    "         if phrase(2 words long) in phrase table:\n",
    "             for all the phrase translations in phrase table:\n",
    "                 if phrase translation in bigram model:\n",
    "                     probability of translation =phrase table probability*bigram probability\n",
    "\n",
    "It is also possible that even the 2-word-long phrase does not exist in phrase table, in this case, the word is translated simply using the previous method(choosing the largest p(e|f)\n",
    "    \n",
    "In the above algorithm, we assumed that if the phrase exists in the phrase table, it exists in the trigram or bigram model. It is also possible that the phrase does not exist in trigram or bigram model. \n",
    "For the 3-word-long phrase, if it does not exist in trigram model, we will use the product of two bigram probabilities to represent the probability of the 3 words. For the 2-word-long phrase, if it does not exist in bigram, we will use two unigram probabilities. \n",
    "\n",
    "The above algorithm is pretty much my understanding of translation by phrase. As it contains too many \"if\"s, when I tried to implement it, it becomes too messy, so I only wrote down my thoughts here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9128b7aae2d2774daf78ff769c0ad33d9ee5e496b0f6baa374285ce2"
   },
   "source": [
    "## Discussion (5% of marks)\n",
    "\n",
    "What conclusions can you make about CLIR, or more generally translation and IR? Overall what approaches worked and what ones didn't? What insights do you have from doing this, put another way, if you were to start again, what approach would you take and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "ea3fcf7b853896ded4b80e969152ae261e3b8c4039f3b998c27f192d"
   },
   "source": [
    "```\n",
    "There are several conclusions I learned from this assignment.\n",
    "\n",
    "Firstly, the capacity of training data and the computing resource available is crucial for machine translation. Our first translation model is built based on translation by word. However, even we are only considering single words, there are still many words in the queries that did not appear in the training data, which means these words are impossible to get translated. On the other hand, if we have more training data, and computing resource allows us to train the model more times, then it is highly likely that we could get better alignments for the existing words and also the size of vocabulary covered would be larger.\n",
    "\n",
    "Secondly, it is important to take the order of words into consideration during translation. Sometimes one words would correspond to more than one words, and in this case, word by word translation would be meaningless. However, if the sentence is translated by phrases, then although one word in the phrase is not very common and of low probability, the phrase as a whole has high probability, and in this way it is likely to get the whole phrase correct.\n",
    "\n",
    "Nonetheless, although translating by phrase could be useful, it highly relies on the training data. If the training dataset is too small, many phrases in the test data would not be covered, and there would be no way but use word by word translation.\n",
    "\n",
    "Thirdly, the BM25 model assumes common words carry less information, which I found is a little questionable. As our results above shown, if the query includes words like where or when, which are quite common words inside the document collection, these words would be given less weights. If the query is \"where is the best Japanese restaurant\" and one document contains \"Japanese restaurants\" many times, but the word \"where\" is missing, this document is likely to be ranked high in the query results, but is unlikely to be the correct result since there is probably no location information.\n",
    "\n",
    "Finally, BM25 also treats queries as a bag of words, which will obiviously affect the performance. If both documents contain the words in the query, one in the same order as the query, another in arbitrary order, they will be ranked the same using BM25 model. From experience, we know the first one is actually better.\n",
    "\n",
    "If I were to start again, I would do the translation part using phrase table and trigram language model, given that there is enough data to get a phrase table of good quality. By good quality I mean most of the phrases in the test data would exist in the phrase table. If the training dataset is large enough, it is reasonable to assume that most of phrases in the test set has corresponding entries in the phrase table and trigram language model.\n",
    "\n",
    "During the querying phase, sometimes there are also phrases in the queries, if we could search for these phrases in the documents as a whole, this would increase the accuracy of the result. We coud segment the queries in various ways and compare the returned results and choose the best ones.\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
